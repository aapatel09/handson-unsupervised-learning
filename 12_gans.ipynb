{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re\n",
    "import pickle, gzip, datetime\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "'''Algos'''\n",
    "import lightgbm as lgb\n",
    "\n",
    "'''TensorFlow and Keras'''\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers import LeakyReLU, Reshape, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras.layers import Embedding, Flatten, dot\n",
    "from keras import regularizers\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "current_path = os.getcwd()\n",
    "file = os.path.sep.join(['', 'datasets', 'mnist_data', 'mnist.pkl.gz'])\n",
    "f = gzip.open(current_path+file, 'rb')\n",
    "train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_validation, y_validation = validation_set[0], validation_set[1]\n",
    "X_test, y_test = test_set[0], test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_keras = X_train.reshape(50000,28,28,1)\n",
    "X_validation_keras = X_validation.reshape(10000,28,28,1)\n",
    "X_test_keras = X_test.reshape(10000,28,28,1)\n",
    "\n",
    "y_train_keras = to_categorical(y_train)\n",
    "y_validation_keras = to_categorical(y_validation)\n",
    "y_test_keras = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Pandas DataFrames from the datasets\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train),len(X_train)+len(X_validation))\n",
    "test_index = range(len(X_train)+len(X_validation),len(X_train)+ \\\n",
    "                   len(X_validation)+len(X_test))\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
    "y_train = pd.Series(data=y_train,index=train_index)\n",
    "\n",
    "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
    "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
    "y_test = pd.Series(data=y_test,index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_digit(X, y, example):\n",
    "    label = y.loc[example]\n",
    "    image = X.loc[example,:].values.reshape([28,28])\n",
    "    plt.title('Example: %d  Label: %d' % (example, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the first digit\n",
    "view_digit(X_train, y_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Confirm use of GPU\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else: print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 15s 304us/step - loss: 0.1865 - acc: 0.9422 - val_loss: 0.0393 - val_acc: 0.9885\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 14s 281us/step - loss: 0.0687 - acc: 0.9796 - val_loss: 0.0360 - val_acc: 0.9895\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 14s 274us/step - loss: 0.0546 - acc: 0.9826 - val_loss: 0.0410 - val_acc: 0.9877\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 14s 274us/step - loss: 0.0475 - acc: 0.9860 - val_loss: 0.0307 - val_acc: 0.9914\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 14s 279us/step - loss: 0.0382 - acc: 0.9884 - val_loss: 0.0300 - val_acc: 0.9923\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 14s 278us/step - loss: 0.0357 - acc: 0.9891 - val_loss: 0.0229 - val_acc: 0.9937\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 18s 362us/step - loss: 0.0334 - acc: 0.9897 - val_loss: 0.0276 - val_acc: 0.9923\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 17s 340us/step - loss: 0.0288 - acc: 0.9909 - val_loss: 0.0238 - val_acc: 0.9931\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 17s 331us/step - loss: 0.0286 - acc: 0.9912 - val_loss: 0.0256 - val_acc: 0.9935\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 16s 315us/step - loss: 0.0248 - acc: 0.9924 - val_loss: 0.0345 - val_acc: 0.9921\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 16s 329us/step - loss: 0.0248 - acc: 0.9923 - val_loss: 0.0315 - val_acc: 0.9934\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 16s 321us/step - loss: 0.0247 - acc: 0.9926 - val_loss: 0.0267 - val_acc: 0.9929\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 17s 344us/step - loss: 0.0219 - acc: 0.9931 - val_loss: 0.0253 - val_acc: 0.9939\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 0.0225 - acc: 0.9929 - val_loss: 0.0312 - val_acc: 0.9939\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 0.0225 - acc: 0.9930 - val_loss: 0.0274 - val_acc: 0.9942\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 14s 280us/step - loss: 0.0218 - acc: 0.9933 - val_loss: 0.0335 - val_acc: 0.9925\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 14s 279us/step - loss: 0.0194 - acc: 0.9941 - val_loss: 0.0306 - val_acc: 0.9930\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 14s 272us/step - loss: 0.0217 - acc: 0.9937 - val_loss: 0.0302 - val_acc: 0.9934\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 13s 268us/step - loss: 0.0203 - acc: 0.9937 - val_loss: 0.0237 - val_acc: 0.9947\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 14s 283us/step - loss: 0.0195 - acc: 0.9943 - val_loss: 0.0266 - val_acc: 0.9941\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 14s 275us/step - loss: 0.0208 - acc: 0.9941 - val_loss: 0.0317 - val_acc: 0.9936\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0190 - acc: 0.9948 - val_loss: 0.0261 - val_acc: 0.9956\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0184 - acc: 0.9945 - val_loss: 0.0293 - val_acc: 0.9943\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 14s 284us/step - loss: 0.0176 - acc: 0.9948 - val_loss: 0.0287 - val_acc: 0.9947\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 17s 339us/step - loss: 0.0185 - acc: 0.9947 - val_loss: 0.0384 - val_acc: 0.9926\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 15s 299us/step - loss: 0.0167 - acc: 0.9955 - val_loss: 0.0352 - val_acc: 0.9932\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 14s 290us/step - loss: 0.0191 - acc: 0.9945 - val_loss: 0.0347 - val_acc: 0.9940\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 14s 278us/step - loss: 0.0183 - acc: 0.9952 - val_loss: 0.0329 - val_acc: 0.9938\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0178 - acc: 0.9947 - val_loss: 0.0378 - val_acc: 0.9939\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 14s 275us/step - loss: 0.0173 - acc: 0.9949 - val_loss: 0.0262 - val_acc: 0.9951\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0188 - acc: 0.9949 - val_loss: 0.0306 - val_acc: 0.9937\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 14s 276us/step - loss: 0.0210 - acc: 0.9940 - val_loss: 0.0290 - val_acc: 0.9940\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 14s 276us/step - loss: 0.0179 - acc: 0.9950 - val_loss: 0.0381 - val_acc: 0.9945\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0202 - acc: 0.9942 - val_loss: 0.0353 - val_acc: 0.9942\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 14s 277us/step - loss: 0.0176 - acc: 0.9954 - val_loss: 0.0384 - val_acc: 0.9938\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0155 - acc: 0.9957 - val_loss: 0.0384 - val_acc: 0.9931\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0207 - acc: 0.9943 - val_loss: 0.0384 - val_acc: 0.9919\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0222 - acc: 0.9943 - val_loss: 0.0399 - val_acc: 0.9934\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0181 - acc: 0.9951 - val_loss: 0.0381 - val_acc: 0.9938\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 14s 271us/step - loss: 0.0148 - acc: 0.9956 - val_loss: 0.0442 - val_acc: 0.9933\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0383 - val_acc: 0.9937\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 13s 268us/step - loss: 0.0191 - acc: 0.9947 - val_loss: 0.0382 - val_acc: 0.9934\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0207 - acc: 0.9949 - val_loss: 0.0373 - val_acc: 0.9942\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 13s 268us/step - loss: 0.0184 - acc: 0.9952 - val_loss: 0.0356 - val_acc: 0.9943\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0427 - val_acc: 0.9931\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0209 - acc: 0.9948 - val_loss: 0.0409 - val_acc: 0.9924\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 13s 269us/step - loss: 0.0146 - acc: 0.9962 - val_loss: 0.0410 - val_acc: 0.9937\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0216 - acc: 0.9944 - val_loss: 0.0332 - val_acc: 0.9946\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 13s 268us/step - loss: 0.0188 - acc: 0.9955 - val_loss: 0.0423 - val_acc: 0.9943\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0171 - acc: 0.9955 - val_loss: 0.0335 - val_acc: 0.9947\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0199 - acc: 0.9955 - val_loss: 0.0379 - val_acc: 0.9945\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0333 - val_acc: 0.9954\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0212 - acc: 0.9953 - val_loss: 0.0376 - val_acc: 0.9935\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 13s 269us/step - loss: 0.0191 - acc: 0.9955 - val_loss: 0.0365 - val_acc: 0.9935\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 13s 268us/step - loss: 0.0206 - acc: 0.9956 - val_loss: 0.0340 - val_acc: 0.9933\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0186 - acc: 0.9956 - val_loss: 0.0390 - val_acc: 0.9947\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0466 - val_acc: 0.9934\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0212 - acc: 0.9951 - val_loss: 0.0394 - val_acc: 0.9947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0201 - acc: 0.9953 - val_loss: 0.0512 - val_acc: 0.9935\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0214 - acc: 0.9957 - val_loss: 0.0368 - val_acc: 0.9940\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0190 - acc: 0.9957 - val_loss: 0.0449 - val_acc: 0.9944\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 13s 265us/step - loss: 0.0206 - acc: 0.9956 - val_loss: 0.0434 - val_acc: 0.9933\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 13s 265us/step - loss: 0.0209 - acc: 0.9949 - val_loss: 0.0444 - val_acc: 0.9945\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.0468 - val_acc: 0.9933\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 13s 269us/step - loss: 0.0200 - acc: 0.9953 - val_loss: 0.0469 - val_acc: 0.9933\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0203 - acc: 0.9956 - val_loss: 0.0533 - val_acc: 0.9933\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0190 - acc: 0.9957 - val_loss: 0.0428 - val_acc: 0.9950\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0407 - val_acc: 0.9952\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0181 - acc: 0.9956 - val_loss: 0.0424 - val_acc: 0.9942\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0220 - acc: 0.9951 - val_loss: 0.0373 - val_acc: 0.9938\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 14s 284us/step - loss: 0.0163 - acc: 0.9963 - val_loss: 0.0429 - val_acc: 0.9952\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 0.0213 - acc: 0.9952 - val_loss: 0.0419 - val_acc: 0.9943\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 14s 282us/step - loss: 0.0196 - acc: 0.9960 - val_loss: 0.0476 - val_acc: 0.9939\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.0240 - acc: 0.9949 - val_loss: 0.0442 - val_acc: 0.9933\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 14s 281us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0499 - val_acc: 0.9932\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 14s 275us/step - loss: 0.0221 - acc: 0.9955 - val_loss: 0.0504 - val_acc: 0.9936\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0248 - acc: 0.9956 - val_loss: 0.0493 - val_acc: 0.9942\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 0.0207 - acc: 0.9958 - val_loss: 0.0449 - val_acc: 0.9941\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 14s 274us/step - loss: 0.0196 - acc: 0.9957 - val_loss: 0.0396 - val_acc: 0.9939\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 14s 271us/step - loss: 0.0253 - acc: 0.9952 - val_loss: 0.0435 - val_acc: 0.9941\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 13s 268us/step - loss: 0.0212 - acc: 0.9958 - val_loss: 0.0437 - val_acc: 0.9947\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0227 - acc: 0.9958 - val_loss: 0.0393 - val_acc: 0.9939\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.0200 - acc: 0.9956 - val_loss: 0.0714 - val_acc: 0.9928\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 14s 276us/step - loss: 0.0253 - acc: 0.9951 - val_loss: 0.0395 - val_acc: 0.9942\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0270 - acc: 0.9949 - val_loss: 0.0385 - val_acc: 0.9950\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 14s 270us/step - loss: 0.0243 - acc: 0.9959 - val_loss: 0.0573 - val_acc: 0.9945\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 14s 275us/step - loss: 0.0221 - acc: 0.9956 - val_loss: 0.0424 - val_acc: 0.9953\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 13s 269us/step - loss: 0.0181 - acc: 0.9963 - val_loss: 0.0411 - val_acc: 0.9944\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 13s 266us/step - loss: 0.0274 - acc: 0.9950 - val_loss: 0.0409 - val_acc: 0.9941\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 13s 264us/step - loss: 0.0261 - acc: 0.9952 - val_loss: 0.0406 - val_acc: 0.9951\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 13s 265us/step - loss: 0.0273 - acc: 0.9954 - val_loss: 0.0585 - val_acc: 0.9931\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 13s 265us/step - loss: 0.0290 - acc: 0.9949 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 13s 265us/step - loss: 0.0304 - acc: 0.9948 - val_loss: 0.0475 - val_acc: 0.9936\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 13s 264us/step - loss: 0.0247 - acc: 0.9959 - val_loss: 0.0433 - val_acc: 0.9947\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 13s 265us/step - loss: 0.0243 - acc: 0.9956 - val_loss: 0.0348 - val_acc: 0.9955\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 13s 264us/step - loss: 0.0257 - acc: 0.9949 - val_loss: 0.0393 - val_acc: 0.9956\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0257 - acc: 0.9954 - val_loss: 0.0522 - val_acc: 0.9943\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 0.0230 - acc: 0.9951 - val_loss: 0.0435 - val_acc: 0.9945\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0246 - acc: 0.9959 - val_loss: 0.0531 - val_acc: 0.9948\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0284 - acc: 0.9955 - val_loss: 0.0445 - val_acc: 0.9944\n"
     ]
    }
   ],
   "source": [
    "# Train CNN\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn_history = model.fit(X_train_keras, y_train_keras, \n",
    "          validation_data=(X_validation_keras, y_validation_keras), \\\n",
    "          epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "print(cnn_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Final Accuracy 0.99554\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracy of CNN\n",
    "print(\"CNN Final Accuracy\", cnn_history.history['acc'][-1])\n",
    "pd.Series(cnn_history.history['acc']).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "        \n",
    "    def generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8, \\\n",
    "                  window=5, input_dim=100, output_depth=1):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=input_dim))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "        \n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(output_depth, window, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator(self, depth=64, dropout=0.3, alpha=0.3):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', \\\n",
    "                        optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', \\\n",
    "                        optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.AM\n",
    "        \n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self, x_train):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = x_train\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            \n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], \\\n",
    "                                                      a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, \\\n",
    "                        samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, \\\n",
    "                    noise=None, step=0):\n",
    "        current_path = os.getcwd()\n",
    "        file = os.path.sep.join(['', 'images', 'chapter12', 'synthetic_mnist', ''])\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(current_path+file+filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.693521, acc: 0.482422]  [A loss: 0.999521, acc: 0.000000]\n",
      "1: [D loss: 0.665768, acc: 0.500000]  [A loss: 0.970452, acc: 0.000000]\n",
      "2: [D loss: 0.611437, acc: 0.998047]  [A loss: 1.020722, acc: 0.000000]\n",
      "3: [D loss: 0.500135, acc: 0.927734]  [A loss: 1.384539, acc: 0.000000]\n",
      "4: [D loss: 0.549321, acc: 0.986328]  [A loss: 0.825565, acc: 0.042969]\n",
      "5: [D loss: 0.632985, acc: 0.500000]  [A loss: 1.219994, acc: 0.000000]\n",
      "6: [D loss: 0.455792, acc: 0.976562]  [A loss: 1.083419, acc: 0.000000]\n",
      "7: [D loss: 0.464011, acc: 0.734375]  [A loss: 1.222585, acc: 0.000000]\n",
      "8: [D loss: 0.477642, acc: 0.580078]  [A loss: 1.344975, acc: 0.000000]\n",
      "9: [D loss: 0.444870, acc: 0.888672]  [A loss: 1.259172, acc: 0.000000]\n",
      "10: [D loss: 0.413312, acc: 0.783203]  [A loss: 1.416222, acc: 0.000000]\n",
      "11: [D loss: 0.375589, acc: 0.990234]  [A loss: 1.259633, acc: 0.000000]\n",
      "12: [D loss: 0.369426, acc: 0.830078]  [A loss: 1.647612, acc: 0.000000]\n",
      "13: [D loss: 0.359703, acc: 1.000000]  [A loss: 1.196602, acc: 0.000000]\n",
      "14: [D loss: 0.380683, acc: 0.730469]  [A loss: 1.765239, acc: 0.000000]\n",
      "15: [D loss: 0.356416, acc: 0.986328]  [A loss: 1.242365, acc: 0.000000]\n",
      "16: [D loss: 0.302566, acc: 0.914062]  [A loss: 1.559030, acc: 0.000000]\n",
      "17: [D loss: 0.275024, acc: 0.992188]  [A loss: 1.404575, acc: 0.000000]\n",
      "18: [D loss: 0.290450, acc: 0.902344]  [A loss: 1.778065, acc: 0.000000]\n",
      "19: [D loss: 0.273768, acc: 0.998047]  [A loss: 1.366044, acc: 0.003906]\n",
      "20: [D loss: 0.292233, acc: 0.892578]  [A loss: 1.955865, acc: 0.000000]\n",
      "21: [D loss: 0.284689, acc: 0.988281]  [A loss: 1.315304, acc: 0.015625]\n",
      "22: [D loss: 0.288285, acc: 0.871094]  [A loss: 1.929926, acc: 0.000000]\n",
      "23: [D loss: 0.246152, acc: 0.984375]  [A loss: 1.350023, acc: 0.011719]\n",
      "24: [D loss: 0.341205, acc: 0.816406]  [A loss: 2.064317, acc: 0.000000]\n",
      "25: [D loss: 0.272492, acc: 0.964844]  [A loss: 1.275901, acc: 0.027344]\n",
      "26: [D loss: 0.304704, acc: 0.861328]  [A loss: 1.827297, acc: 0.000000]\n",
      "27: [D loss: 0.214094, acc: 0.992188]  [A loss: 1.368664, acc: 0.027344]\n",
      "28: [D loss: 0.301631, acc: 0.869141]  [A loss: 1.960537, acc: 0.000000]\n",
      "29: [D loss: 0.209049, acc: 0.980469]  [A loss: 1.337277, acc: 0.019531]\n",
      "30: [D loss: 0.297427, acc: 0.876953]  [A loss: 2.062126, acc: 0.000000]\n",
      "31: [D loss: 0.223268, acc: 0.972656]  [A loss: 1.158382, acc: 0.117188]\n",
      "32: [D loss: 0.423783, acc: 0.748047]  [A loss: 2.295713, acc: 0.000000]\n",
      "33: [D loss: 0.310145, acc: 0.898438]  [A loss: 0.991124, acc: 0.218750]\n",
      "34: [D loss: 0.435454, acc: 0.716797]  [A loss: 1.841018, acc: 0.000000]\n",
      "35: [D loss: 0.225962, acc: 0.970703]  [A loss: 1.137355, acc: 0.101562]\n",
      "36: [D loss: 0.351119, acc: 0.810547]  [A loss: 1.838434, acc: 0.000000]\n",
      "37: [D loss: 0.253217, acc: 0.960938]  [A loss: 1.299116, acc: 0.050781]\n",
      "38: [D loss: 0.367011, acc: 0.785156]  [A loss: 1.981523, acc: 0.000000]\n",
      "39: [D loss: 0.258743, acc: 0.958984]  [A loss: 1.150618, acc: 0.105469]\n",
      "40: [D loss: 0.399906, acc: 0.755859]  [A loss: 2.131672, acc: 0.000000]\n",
      "41: [D loss: 0.270999, acc: 0.945312]  [A loss: 1.079068, acc: 0.144531]\n",
      "42: [D loss: 0.391744, acc: 0.753906]  [A loss: 2.001009, acc: 0.000000]\n",
      "43: [D loss: 0.288839, acc: 0.951172]  [A loss: 1.042905, acc: 0.148438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44: [D loss: 0.445127, acc: 0.726562]  [A loss: 2.053391, acc: 0.000000]\n",
      "45: [D loss: 0.336249, acc: 0.916016]  [A loss: 0.965388, acc: 0.179688]\n",
      "46: [D loss: 0.496942, acc: 0.667969]  [A loss: 1.989622, acc: 0.000000]\n",
      "47: [D loss: 0.323396, acc: 0.935547]  [A loss: 1.008242, acc: 0.132812]\n",
      "48: [D loss: 0.494301, acc: 0.644531]  [A loss: 1.953891, acc: 0.000000]\n",
      "49: [D loss: 0.337476, acc: 0.925781]  [A loss: 1.084003, acc: 0.054688]\n",
      "50: [D loss: 0.461106, acc: 0.669922]  [A loss: 1.821316, acc: 0.000000]\n",
      "51: [D loss: 0.330568, acc: 0.927734]  [A loss: 1.096790, acc: 0.062500]\n",
      "52: [D loss: 0.474425, acc: 0.636719]  [A loss: 1.908188, acc: 0.000000]\n",
      "53: [D loss: 0.358409, acc: 0.937500]  [A loss: 1.058173, acc: 0.054688]\n",
      "54: [D loss: 0.522793, acc: 0.580078]  [A loss: 1.973981, acc: 0.000000]\n",
      "55: [D loss: 0.386919, acc: 0.916016]  [A loss: 0.951340, acc: 0.113281]\n",
      "56: [D loss: 0.607943, acc: 0.542969]  [A loss: 1.959654, acc: 0.000000]\n",
      "57: [D loss: 0.365952, acc: 0.939453]  [A loss: 1.004144, acc: 0.042969]\n",
      "58: [D loss: 0.534914, acc: 0.548828]  [A loss: 1.801396, acc: 0.000000]\n",
      "59: [D loss: 0.377258, acc: 0.943359]  [A loss: 1.058104, acc: 0.046875]\n",
      "60: [D loss: 0.534864, acc: 0.546875]  [A loss: 1.877985, acc: 0.000000]\n",
      "61: [D loss: 0.374287, acc: 0.945312]  [A loss: 1.064356, acc: 0.042969]\n",
      "62: [D loss: 0.539248, acc: 0.541016]  [A loss: 1.959086, acc: 0.000000]\n",
      "63: [D loss: 0.389173, acc: 0.937500]  [A loss: 0.961619, acc: 0.113281]\n",
      "64: [D loss: 0.632298, acc: 0.513672]  [A loss: 2.021201, acc: 0.000000]\n",
      "65: [D loss: 0.404889, acc: 0.927734]  [A loss: 0.910208, acc: 0.113281]\n",
      "66: [D loss: 0.599282, acc: 0.517578]  [A loss: 1.856922, acc: 0.000000]\n",
      "67: [D loss: 0.388871, acc: 0.968750]  [A loss: 1.067506, acc: 0.035156]\n",
      "68: [D loss: 0.561766, acc: 0.521484]  [A loss: 1.946566, acc: 0.000000]\n",
      "69: [D loss: 0.401675, acc: 0.953125]  [A loss: 1.009022, acc: 0.035156]\n",
      "70: [D loss: 0.578040, acc: 0.521484]  [A loss: 1.963296, acc: 0.000000]\n",
      "71: [D loss: 0.413913, acc: 0.929688]  [A loss: 0.941039, acc: 0.074219]\n",
      "72: [D loss: 0.622925, acc: 0.505859]  [A loss: 2.044306, acc: 0.000000]\n",
      "73: [D loss: 0.425586, acc: 0.943359]  [A loss: 0.823982, acc: 0.253906]\n",
      "74: [D loss: 0.651816, acc: 0.500000]  [A loss: 1.970463, acc: 0.000000]\n",
      "75: [D loss: 0.423850, acc: 0.929688]  [A loss: 0.888307, acc: 0.152344]\n",
      "76: [D loss: 0.626796, acc: 0.507812]  [A loss: 1.945859, acc: 0.000000]\n",
      "77: [D loss: 0.425632, acc: 0.931641]  [A loss: 0.926598, acc: 0.097656]\n",
      "78: [D loss: 0.616179, acc: 0.505859]  [A loss: 1.949060, acc: 0.000000]\n",
      "79: [D loss: 0.446005, acc: 0.912109]  [A loss: 0.917575, acc: 0.093750]\n",
      "80: [D loss: 0.638734, acc: 0.501953]  [A loss: 2.006495, acc: 0.000000]\n",
      "81: [D loss: 0.454509, acc: 0.900391]  [A loss: 0.838777, acc: 0.183594]\n",
      "82: [D loss: 0.677665, acc: 0.503906]  [A loss: 2.001352, acc: 0.000000]\n",
      "83: [D loss: 0.459130, acc: 0.894531]  [A loss: 0.806081, acc: 0.257812]\n",
      "84: [D loss: 0.684521, acc: 0.511719]  [A loss: 1.938465, acc: 0.000000]\n",
      "85: [D loss: 0.473242, acc: 0.884766]  [A loss: 0.756336, acc: 0.390625]\n",
      "86: [D loss: 0.675340, acc: 0.500000]  [A loss: 1.860257, acc: 0.000000]\n",
      "87: [D loss: 0.473091, acc: 0.894531]  [A loss: 0.827226, acc: 0.238281]\n",
      "88: [D loss: 0.650986, acc: 0.509766]  [A loss: 1.871467, acc: 0.000000]\n",
      "89: [D loss: 0.484671, acc: 0.876953]  [A loss: 0.818917, acc: 0.261719]\n",
      "90: [D loss: 0.679457, acc: 0.503906]  [A loss: 1.929250, acc: 0.000000]\n",
      "91: [D loss: 0.495831, acc: 0.871094]  [A loss: 0.804734, acc: 0.273438]\n",
      "92: [D loss: 0.685880, acc: 0.509766]  [A loss: 1.910521, acc: 0.000000]\n",
      "93: [D loss: 0.509364, acc: 0.863281]  [A loss: 0.768211, acc: 0.378906]\n",
      "94: [D loss: 0.704093, acc: 0.505859]  [A loss: 1.895359, acc: 0.000000]\n",
      "95: [D loss: 0.502887, acc: 0.861328]  [A loss: 0.767634, acc: 0.367188]\n",
      "96: [D loss: 0.696305, acc: 0.501953]  [A loss: 1.892239, acc: 0.000000]\n",
      "97: [D loss: 0.496321, acc: 0.851562]  [A loss: 0.822751, acc: 0.250000]\n",
      "98: [D loss: 0.693539, acc: 0.503906]  [A loss: 1.816223, acc: 0.000000]\n",
      "99: [D loss: 0.521528, acc: 0.841797]  [A loss: 0.779058, acc: 0.339844]\n",
      "100: [D loss: 0.714109, acc: 0.498047]  [A loss: 1.920454, acc: 0.000000]\n",
      "101: [D loss: 0.531829, acc: 0.843750]  [A loss: 0.746225, acc: 0.425781]\n",
      "102: [D loss: 0.721483, acc: 0.503906]  [A loss: 1.830690, acc: 0.000000]\n",
      "103: [D loss: 0.539449, acc: 0.835938]  [A loss: 0.719714, acc: 0.480469]\n",
      "104: [D loss: 0.705497, acc: 0.503906]  [A loss: 1.820098, acc: 0.000000]\n",
      "105: [D loss: 0.523950, acc: 0.839844]  [A loss: 0.771828, acc: 0.324219]\n",
      "106: [D loss: 0.673732, acc: 0.500000]  [A loss: 1.722564, acc: 0.000000]\n",
      "107: [D loss: 0.516765, acc: 0.853516]  [A loss: 0.827836, acc: 0.257812]\n",
      "108: [D loss: 0.667640, acc: 0.507812]  [A loss: 1.913069, acc: 0.000000]\n",
      "109: [D loss: 0.539605, acc: 0.814453]  [A loss: 0.671505, acc: 0.546875]\n",
      "110: [D loss: 0.758013, acc: 0.498047]  [A loss: 1.862233, acc: 0.000000]\n",
      "111: [D loss: 0.550917, acc: 0.791016]  [A loss: 0.677615, acc: 0.542969]\n",
      "112: [D loss: 0.707529, acc: 0.501953]  [A loss: 1.543393, acc: 0.000000]\n",
      "113: [D loss: 0.538862, acc: 0.810547]  [A loss: 0.886196, acc: 0.167969]\n",
      "114: [D loss: 0.648244, acc: 0.509766]  [A loss: 1.674698, acc: 0.000000]\n",
      "115: [D loss: 0.521936, acc: 0.857422]  [A loss: 0.815436, acc: 0.250000]\n",
      "116: [D loss: 0.694191, acc: 0.505859]  [A loss: 1.881524, acc: 0.000000]\n",
      "117: [D loss: 0.556157, acc: 0.814453]  [A loss: 0.646534, acc: 0.628906]\n",
      "118: [D loss: 0.731801, acc: 0.498047]  [A loss: 1.712939, acc: 0.000000]\n",
      "119: [D loss: 0.559137, acc: 0.802734]  [A loss: 0.756438, acc: 0.339844]\n",
      "120: [D loss: 0.687232, acc: 0.496094]  [A loss: 1.770465, acc: 0.000000]\n",
      "121: [D loss: 0.548293, acc: 0.787109]  [A loss: 0.735283, acc: 0.414062]\n",
      "122: [D loss: 0.713160, acc: 0.503906]  [A loss: 1.747444, acc: 0.000000]\n",
      "123: [D loss: 0.555019, acc: 0.812500]  [A loss: 0.707079, acc: 0.476562]\n",
      "124: [D loss: 0.706562, acc: 0.500000]  [A loss: 1.745510, acc: 0.000000]\n",
      "125: [D loss: 0.547952, acc: 0.812500]  [A loss: 0.699004, acc: 0.460938]\n",
      "126: [D loss: 0.722538, acc: 0.500000]  [A loss: 1.682432, acc: 0.000000]\n",
      "127: [D loss: 0.554847, acc: 0.833984]  [A loss: 0.733832, acc: 0.406250]\n",
      "128: [D loss: 0.696084, acc: 0.501953]  [A loss: 1.734585, acc: 0.000000]\n",
      "129: [D loss: 0.540134, acc: 0.814453]  [A loss: 0.723128, acc: 0.437500]\n",
      "130: [D loss: 0.718383, acc: 0.498047]  [A loss: 1.846390, acc: 0.000000]\n",
      "131: [D loss: 0.554424, acc: 0.767578]  [A loss: 0.639586, acc: 0.675781]\n",
      "132: [D loss: 0.736830, acc: 0.500000]  [A loss: 1.624823, acc: 0.000000]\n",
      "133: [D loss: 0.528095, acc: 0.851562]  [A loss: 0.854395, acc: 0.199219]\n",
      "134: [D loss: 0.659374, acc: 0.503906]  [A loss: 1.757889, acc: 0.000000]\n",
      "135: [D loss: 0.548470, acc: 0.816406]  [A loss: 0.726724, acc: 0.441406]\n",
      "136: [D loss: 0.706480, acc: 0.501953]  [A loss: 1.800133, acc: 0.000000]\n",
      "137: [D loss: 0.559634, acc: 0.785156]  [A loss: 0.694195, acc: 0.519531]\n",
      "138: [D loss: 0.731350, acc: 0.505859]  [A loss: 1.776523, acc: 0.000000]\n",
      "139: [D loss: 0.554936, acc: 0.816406]  [A loss: 0.677772, acc: 0.570312]\n",
      "140: [D loss: 0.752602, acc: 0.500000]  [A loss: 1.805784, acc: 0.000000]\n",
      "141: [D loss: 0.585220, acc: 0.746094]  [A loss: 0.683889, acc: 0.515625]\n",
      "142: [D loss: 0.695673, acc: 0.498047]  [A loss: 1.477056, acc: 0.000000]\n",
      "143: [D loss: 0.528447, acc: 0.845703]  [A loss: 0.914842, acc: 0.121094]\n",
      "144: [D loss: 0.673798, acc: 0.515625]  [A loss: 1.897854, acc: 0.000000]\n",
      "145: [D loss: 0.577186, acc: 0.712891]  [A loss: 0.612899, acc: 0.707031]\n",
      "146: [D loss: 0.752759, acc: 0.500000]  [A loss: 1.596896, acc: 0.000000]\n",
      "147: [D loss: 0.548473, acc: 0.828125]  [A loss: 0.776614, acc: 0.300781]\n",
      "148: [D loss: 0.678314, acc: 0.507812]  [A loss: 1.840274, acc: 0.000000]\n",
      "149: [D loss: 0.555319, acc: 0.779297]  [A loss: 0.679246, acc: 0.554688]\n",
      "150: [D loss: 0.691180, acc: 0.500000]  [A loss: 1.657702, acc: 0.000000]\n",
      "151: [D loss: 0.555978, acc: 0.814453]  [A loss: 0.758468, acc: 0.359375]\n",
      "152: [D loss: 0.697371, acc: 0.500000]  [A loss: 1.637660, acc: 0.000000]\n",
      "153: [D loss: 0.544022, acc: 0.814453]  [A loss: 0.784599, acc: 0.257812]\n",
      "154: [D loss: 0.695729, acc: 0.509766]  [A loss: 1.771272, acc: 0.000000]\n",
      "155: [D loss: 0.586606, acc: 0.728516]  [A loss: 0.698180, acc: 0.507812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156: [D loss: 0.710699, acc: 0.501953]  [A loss: 1.574044, acc: 0.000000]\n",
      "157: [D loss: 0.558222, acc: 0.763672]  [A loss: 1.029745, acc: 0.042969]\n",
      "158: [D loss: 0.651407, acc: 0.544922]  [A loss: 1.622118, acc: 0.000000]\n",
      "159: [D loss: 0.571437, acc: 0.794922]  [A loss: 0.762057, acc: 0.320312]\n",
      "160: [D loss: 0.689994, acc: 0.503906]  [A loss: 1.723031, acc: 0.000000]\n",
      "161: [D loss: 0.550024, acc: 0.824219]  [A loss: 0.777561, acc: 0.355469]\n",
      "162: [D loss: 0.709070, acc: 0.509766]  [A loss: 1.793669, acc: 0.000000]\n",
      "163: [D loss: 0.622888, acc: 0.648438]  [A loss: 0.646972, acc: 0.660156]\n",
      "164: [D loss: 0.752639, acc: 0.498047]  [A loss: 1.746295, acc: 0.000000]\n",
      "165: [D loss: 0.579979, acc: 0.765625]  [A loss: 0.766270, acc: 0.359375]\n",
      "166: [D loss: 0.697887, acc: 0.509766]  [A loss: 1.563255, acc: 0.000000]\n",
      "167: [D loss: 0.593992, acc: 0.755859]  [A loss: 0.725838, acc: 0.437500]\n",
      "168: [D loss: 0.718403, acc: 0.507812]  [A loss: 1.625949, acc: 0.000000]\n",
      "169: [D loss: 0.595139, acc: 0.693359]  [A loss: 0.955710, acc: 0.093750]\n",
      "170: [D loss: 0.642451, acc: 0.544922]  [A loss: 1.434391, acc: 0.000000]\n",
      "171: [D loss: 0.590404, acc: 0.730469]  [A loss: 0.912852, acc: 0.117188]\n",
      "172: [D loss: 0.662826, acc: 0.531250]  [A loss: 1.770205, acc: 0.000000]\n",
      "173: [D loss: 0.597206, acc: 0.718750]  [A loss: 0.707826, acc: 0.515625]\n",
      "174: [D loss: 0.744751, acc: 0.503906]  [A loss: 1.834554, acc: 0.000000]\n",
      "175: [D loss: 0.626006, acc: 0.646484]  [A loss: 0.592650, acc: 0.753906]\n",
      "176: [D loss: 0.764547, acc: 0.500000]  [A loss: 1.711815, acc: 0.000000]\n",
      "177: [D loss: 0.598117, acc: 0.673828]  [A loss: 0.605633, acc: 0.730469]\n",
      "178: [D loss: 0.741582, acc: 0.511719]  [A loss: 1.568551, acc: 0.000000]\n",
      "179: [D loss: 0.632296, acc: 0.656250]  [A loss: 0.757363, acc: 0.371094]\n",
      "180: [D loss: 0.684607, acc: 0.519531]  [A loss: 1.420085, acc: 0.000000]\n",
      "181: [D loss: 0.607196, acc: 0.683594]  [A loss: 1.008840, acc: 0.066406]\n",
      "182: [D loss: 0.614610, acc: 0.601562]  [A loss: 1.199659, acc: 0.007812]\n",
      "183: [D loss: 0.608841, acc: 0.652344]  [A loss: 1.172212, acc: 0.011719]\n",
      "184: [D loss: 0.638830, acc: 0.585938]  [A loss: 1.409142, acc: 0.000000]\n",
      "185: [D loss: 0.610188, acc: 0.697266]  [A loss: 0.958990, acc: 0.093750]\n",
      "186: [D loss: 0.677314, acc: 0.542969]  [A loss: 2.032130, acc: 0.000000]\n",
      "187: [D loss: 0.619962, acc: 0.646484]  [A loss: 0.462746, acc: 0.910156]\n",
      "188: [D loss: 0.882501, acc: 0.501953]  [A loss: 1.698995, acc: 0.000000]\n",
      "189: [D loss: 0.671041, acc: 0.558594]  [A loss: 0.639979, acc: 0.656250]\n",
      "190: [D loss: 0.733557, acc: 0.507812]  [A loss: 1.565632, acc: 0.003906]\n",
      "191: [D loss: 0.626711, acc: 0.660156]  [A loss: 0.750708, acc: 0.375000]\n",
      "192: [D loss: 0.695303, acc: 0.519531]  [A loss: 1.358856, acc: 0.000000]\n",
      "193: [D loss: 0.618501, acc: 0.710938]  [A loss: 0.822430, acc: 0.289062]\n",
      "194: [D loss: 0.683234, acc: 0.519531]  [A loss: 1.522477, acc: 0.000000]\n",
      "195: [D loss: 0.621329, acc: 0.685547]  [A loss: 0.755105, acc: 0.406250]\n",
      "196: [D loss: 0.719406, acc: 0.515625]  [A loss: 1.546902, acc: 0.000000]\n",
      "197: [D loss: 0.608778, acc: 0.736328]  [A loss: 0.658310, acc: 0.593750]\n",
      "198: [D loss: 0.752845, acc: 0.509766]  [A loss: 1.758878, acc: 0.000000]\n",
      "199: [D loss: 0.650191, acc: 0.570312]  [A loss: 0.600092, acc: 0.734375]\n",
      "200: [D loss: 0.764073, acc: 0.498047]  [A loss: 1.398782, acc: 0.000000]\n",
      "201: [D loss: 0.616652, acc: 0.685547]  [A loss: 0.812779, acc: 0.285156]\n",
      "202: [D loss: 0.683359, acc: 0.507812]  [A loss: 1.344796, acc: 0.000000]\n",
      "203: [D loss: 0.639307, acc: 0.660156]  [A loss: 0.780500, acc: 0.312500]\n",
      "204: [D loss: 0.708244, acc: 0.527344]  [A loss: 1.558649, acc: 0.000000]\n",
      "205: [D loss: 0.642352, acc: 0.630859]  [A loss: 0.642094, acc: 0.664062]\n",
      "206: [D loss: 0.732162, acc: 0.494141]  [A loss: 1.527200, acc: 0.007812]\n",
      "207: [D loss: 0.680097, acc: 0.542969]  [A loss: 0.901115, acc: 0.156250]\n",
      "208: [D loss: 0.700753, acc: 0.511719]  [A loss: 1.284479, acc: 0.000000]\n",
      "209: [D loss: 0.626661, acc: 0.679688]  [A loss: 0.896390, acc: 0.132812]\n",
      "210: [D loss: 0.658455, acc: 0.539062]  [A loss: 1.453535, acc: 0.003906]\n",
      "211: [D loss: 0.652474, acc: 0.599609]  [A loss: 0.914363, acc: 0.132812]\n",
      "212: [D loss: 0.690908, acc: 0.507812]  [A loss: 1.491453, acc: 0.000000]\n",
      "213: [D loss: 0.621174, acc: 0.708984]  [A loss: 0.805078, acc: 0.281250]\n",
      "214: [D loss: 0.721933, acc: 0.505859]  [A loss: 1.928814, acc: 0.000000]\n",
      "215: [D loss: 0.661517, acc: 0.552734]  [A loss: 0.488526, acc: 0.921875]\n",
      "216: [D loss: 0.848581, acc: 0.500000]  [A loss: 1.474973, acc: 0.000000]\n",
      "217: [D loss: 0.665803, acc: 0.601562]  [A loss: 0.663925, acc: 0.566406]\n",
      "218: [D loss: 0.724088, acc: 0.503906]  [A loss: 1.366705, acc: 0.000000]\n",
      "219: [D loss: 0.640862, acc: 0.662109]  [A loss: 0.700258, acc: 0.480469]\n",
      "220: [D loss: 0.737759, acc: 0.503906]  [A loss: 1.263068, acc: 0.003906]\n",
      "221: [D loss: 0.638255, acc: 0.708984]  [A loss: 0.770648, acc: 0.339844]\n",
      "222: [D loss: 0.691504, acc: 0.507812]  [A loss: 1.313984, acc: 0.000000]\n",
      "223: [D loss: 0.645748, acc: 0.644531]  [A loss: 0.737341, acc: 0.386719]\n",
      "224: [D loss: 0.713107, acc: 0.501953]  [A loss: 1.369306, acc: 0.003906]\n",
      "225: [D loss: 0.654274, acc: 0.623047]  [A loss: 0.804411, acc: 0.238281]\n",
      "226: [D loss: 0.690823, acc: 0.513672]  [A loss: 1.359767, acc: 0.000000]\n",
      "227: [D loss: 0.654202, acc: 0.613281]  [A loss: 0.741579, acc: 0.398438]\n",
      "228: [D loss: 0.727543, acc: 0.509766]  [A loss: 1.492866, acc: 0.000000]\n",
      "229: [D loss: 0.656674, acc: 0.625000]  [A loss: 0.622047, acc: 0.703125]\n",
      "230: [D loss: 0.738189, acc: 0.501953]  [A loss: 1.590611, acc: 0.000000]\n",
      "231: [D loss: 0.679936, acc: 0.568359]  [A loss: 0.662978, acc: 0.636719]\n",
      "232: [D loss: 0.765509, acc: 0.496094]  [A loss: 1.217024, acc: 0.000000]\n",
      "233: [D loss: 0.663571, acc: 0.609375]  [A loss: 0.770418, acc: 0.332031]\n",
      "234: [D loss: 0.705718, acc: 0.517578]  [A loss: 1.308636, acc: 0.000000]\n",
      "235: [D loss: 0.651454, acc: 0.658203]  [A loss: 0.735022, acc: 0.375000]\n",
      "236: [D loss: 0.709299, acc: 0.501953]  [A loss: 1.316807, acc: 0.000000]\n",
      "237: [D loss: 0.647520, acc: 0.619141]  [A loss: 0.679689, acc: 0.570312]\n",
      "238: [D loss: 0.723572, acc: 0.501953]  [A loss: 1.360715, acc: 0.000000]\n",
      "239: [D loss: 0.651061, acc: 0.638672]  [A loss: 0.668129, acc: 0.601562]\n",
      "240: [D loss: 0.745947, acc: 0.501953]  [A loss: 1.277670, acc: 0.000000]\n",
      "241: [D loss: 0.654483, acc: 0.654297]  [A loss: 0.725467, acc: 0.425781]\n",
      "242: [D loss: 0.709620, acc: 0.500000]  [A loss: 1.320470, acc: 0.000000]\n",
      "243: [D loss: 0.654537, acc: 0.626953]  [A loss: 0.681863, acc: 0.531250]\n",
      "244: [D loss: 0.717376, acc: 0.498047]  [A loss: 1.239292, acc: 0.000000]\n",
      "245: [D loss: 0.668880, acc: 0.605469]  [A loss: 0.776882, acc: 0.320312]\n",
      "246: [D loss: 0.716429, acc: 0.503906]  [A loss: 1.153014, acc: 0.003906]\n",
      "247: [D loss: 0.650467, acc: 0.675781]  [A loss: 0.775348, acc: 0.296875]\n",
      "248: [D loss: 0.693902, acc: 0.515625]  [A loss: 1.296812, acc: 0.000000]\n",
      "249: [D loss: 0.672152, acc: 0.605469]  [A loss: 0.703713, acc: 0.500000]\n",
      "250: [D loss: 0.728655, acc: 0.503906]  [A loss: 1.377076, acc: 0.000000]\n",
      "251: [D loss: 0.653969, acc: 0.605469]  [A loss: 0.629195, acc: 0.691406]\n",
      "252: [D loss: 0.759268, acc: 0.498047]  [A loss: 1.232879, acc: 0.000000]\n",
      "253: [D loss: 0.676106, acc: 0.583984]  [A loss: 0.672344, acc: 0.566406]\n",
      "254: [D loss: 0.727652, acc: 0.507812]  [A loss: 1.238323, acc: 0.000000]\n",
      "255: [D loss: 0.656990, acc: 0.646484]  [A loss: 0.691754, acc: 0.503906]\n",
      "256: [D loss: 0.703907, acc: 0.500000]  [A loss: 1.123854, acc: 0.019531]\n",
      "257: [D loss: 0.658757, acc: 0.625000]  [A loss: 0.818018, acc: 0.175781]\n",
      "258: [D loss: 0.698413, acc: 0.515625]  [A loss: 1.118067, acc: 0.011719]\n",
      "259: [D loss: 0.645294, acc: 0.650391]  [A loss: 0.855122, acc: 0.160156]\n",
      "260: [D loss: 0.687642, acc: 0.523438]  [A loss: 1.305020, acc: 0.000000]\n",
      "261: [D loss: 0.658934, acc: 0.630859]  [A loss: 0.643552, acc: 0.695312]\n",
      "262: [D loss: 0.735056, acc: 0.496094]  [A loss: 1.426425, acc: 0.000000]\n",
      "263: [D loss: 0.672015, acc: 0.558594]  [A loss: 0.645102, acc: 0.695312]\n",
      "264: [D loss: 0.739015, acc: 0.496094]  [A loss: 1.223205, acc: 0.000000]\n",
      "265: [D loss: 0.679374, acc: 0.583984]  [A loss: 0.698366, acc: 0.457031]\n",
      "266: [D loss: 0.718304, acc: 0.509766]  [A loss: 1.260031, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267: [D loss: 0.661295, acc: 0.626953]  [A loss: 0.653556, acc: 0.652344]\n",
      "268: [D loss: 0.719801, acc: 0.503906]  [A loss: 1.215851, acc: 0.000000]\n",
      "269: [D loss: 0.651974, acc: 0.666016]  [A loss: 0.693946, acc: 0.515625]\n",
      "270: [D loss: 0.710191, acc: 0.509766]  [A loss: 1.253343, acc: 0.000000]\n",
      "271: [D loss: 0.651984, acc: 0.646484]  [A loss: 0.686800, acc: 0.535156]\n",
      "272: [D loss: 0.718245, acc: 0.500000]  [A loss: 1.237920, acc: 0.000000]\n",
      "273: [D loss: 0.656831, acc: 0.630859]  [A loss: 0.719147, acc: 0.425781]\n",
      "274: [D loss: 0.704384, acc: 0.507812]  [A loss: 1.225173, acc: 0.000000]\n",
      "275: [D loss: 0.650785, acc: 0.675781]  [A loss: 0.693159, acc: 0.542969]\n",
      "276: [D loss: 0.719948, acc: 0.505859]  [A loss: 1.280236, acc: 0.000000]\n",
      "277: [D loss: 0.664938, acc: 0.611328]  [A loss: 0.674516, acc: 0.589844]\n",
      "278: [D loss: 0.727638, acc: 0.501953]  [A loss: 1.206526, acc: 0.000000]\n",
      "279: [D loss: 0.676062, acc: 0.587891]  [A loss: 0.722503, acc: 0.429688]\n",
      "280: [D loss: 0.710710, acc: 0.501953]  [A loss: 1.156601, acc: 0.011719]\n",
      "281: [D loss: 0.674670, acc: 0.585938]  [A loss: 0.850348, acc: 0.160156]\n",
      "282: [D loss: 0.700380, acc: 0.498047]  [A loss: 0.983160, acc: 0.007812]\n",
      "283: [D loss: 0.678509, acc: 0.593750]  [A loss: 0.942520, acc: 0.039062]\n",
      "284: [D loss: 0.665357, acc: 0.587891]  [A loss: 0.972995, acc: 0.019531]\n",
      "285: [D loss: 0.673968, acc: 0.570312]  [A loss: 0.989055, acc: 0.007812]\n",
      "286: [D loss: 0.669780, acc: 0.558594]  [A loss: 1.007123, acc: 0.023438]\n",
      "287: [D loss: 0.669282, acc: 0.570312]  [A loss: 0.963723, acc: 0.046875]\n",
      "288: [D loss: 0.661980, acc: 0.609375]  [A loss: 1.109432, acc: 0.000000]\n",
      "289: [D loss: 0.663356, acc: 0.597656]  [A loss: 0.823676, acc: 0.167969]\n",
      "290: [D loss: 0.690986, acc: 0.523438]  [A loss: 1.505647, acc: 0.000000]\n",
      "291: [D loss: 0.681992, acc: 0.539062]  [A loss: 0.610945, acc: 0.722656]\n",
      "292: [D loss: 0.774829, acc: 0.500000]  [A loss: 1.347797, acc: 0.000000]\n",
      "293: [D loss: 0.711113, acc: 0.507812]  [A loss: 0.690883, acc: 0.519531]\n",
      "294: [D loss: 0.706054, acc: 0.498047]  [A loss: 1.120080, acc: 0.011719]\n",
      "295: [D loss: 0.669119, acc: 0.589844]  [A loss: 0.819894, acc: 0.175781]\n",
      "296: [D loss: 0.678131, acc: 0.539062]  [A loss: 1.015483, acc: 0.003906]\n",
      "297: [D loss: 0.682758, acc: 0.556641]  [A loss: 0.839938, acc: 0.125000]\n",
      "298: [D loss: 0.674280, acc: 0.531250]  [A loss: 1.033018, acc: 0.019531]\n",
      "299: [D loss: 0.665070, acc: 0.593750]  [A loss: 0.863427, acc: 0.078125]\n",
      "300: [D loss: 0.675386, acc: 0.531250]  [A loss: 1.071885, acc: 0.003906]\n",
      "301: [D loss: 0.654309, acc: 0.646484]  [A loss: 0.829797, acc: 0.136719]\n",
      "302: [D loss: 0.684836, acc: 0.513672]  [A loss: 1.237809, acc: 0.000000]\n",
      "303: [D loss: 0.664821, acc: 0.630859]  [A loss: 0.626125, acc: 0.714844]\n",
      "304: [D loss: 0.718784, acc: 0.507812]  [A loss: 1.338595, acc: 0.000000]\n",
      "305: [D loss: 0.677118, acc: 0.539062]  [A loss: 0.577769, acc: 0.851562]\n",
      "306: [D loss: 0.749424, acc: 0.500000]  [A loss: 1.186217, acc: 0.003906]\n",
      "307: [D loss: 0.669551, acc: 0.585938]  [A loss: 0.652568, acc: 0.640625]\n",
      "308: [D loss: 0.713810, acc: 0.505859]  [A loss: 1.070796, acc: 0.000000]\n",
      "309: [D loss: 0.661469, acc: 0.626953]  [A loss: 0.730365, acc: 0.371094]\n",
      "310: [D loss: 0.696751, acc: 0.515625]  [A loss: 1.018726, acc: 0.003906]\n",
      "311: [D loss: 0.674674, acc: 0.576172]  [A loss: 0.779795, acc: 0.242188]\n",
      "312: [D loss: 0.684333, acc: 0.507812]  [A loss: 0.983583, acc: 0.031250]\n",
      "313: [D loss: 0.656995, acc: 0.617188]  [A loss: 0.834645, acc: 0.136719]\n",
      "314: [D loss: 0.675221, acc: 0.556641]  [A loss: 1.020659, acc: 0.000000]\n",
      "315: [D loss: 0.665877, acc: 0.601562]  [A loss: 0.842656, acc: 0.156250]\n",
      "316: [D loss: 0.678948, acc: 0.546875]  [A loss: 1.056797, acc: 0.023438]\n",
      "317: [D loss: 0.676999, acc: 0.580078]  [A loss: 0.850096, acc: 0.148438]\n",
      "318: [D loss: 0.699892, acc: 0.523438]  [A loss: 1.088491, acc: 0.019531]\n",
      "319: [D loss: 0.673212, acc: 0.593750]  [A loss: 0.771036, acc: 0.285156]\n",
      "320: [D loss: 0.696359, acc: 0.511719]  [A loss: 1.271812, acc: 0.000000]\n",
      "321: [D loss: 0.683937, acc: 0.548828]  [A loss: 0.652909, acc: 0.613281]\n",
      "322: [D loss: 0.723695, acc: 0.505859]  [A loss: 1.340737, acc: 0.000000]\n",
      "323: [D loss: 0.704963, acc: 0.521484]  [A loss: 0.627759, acc: 0.718750]\n",
      "324: [D loss: 0.726461, acc: 0.501953]  [A loss: 1.025372, acc: 0.011719]\n",
      "325: [D loss: 0.673513, acc: 0.595703]  [A loss: 0.721977, acc: 0.410156]\n",
      "326: [D loss: 0.691836, acc: 0.521484]  [A loss: 0.992297, acc: 0.019531]\n",
      "327: [D loss: 0.654896, acc: 0.640625]  [A loss: 0.765672, acc: 0.250000]\n",
      "328: [D loss: 0.695058, acc: 0.542969]  [A loss: 1.062325, acc: 0.003906]\n",
      "329: [D loss: 0.679166, acc: 0.554688]  [A loss: 0.774052, acc: 0.269531]\n",
      "330: [D loss: 0.689328, acc: 0.523438]  [A loss: 0.995967, acc: 0.015625]\n",
      "331: [D loss: 0.679188, acc: 0.587891]  [A loss: 0.774940, acc: 0.304688]\n",
      "332: [D loss: 0.689928, acc: 0.525391]  [A loss: 1.045949, acc: 0.007812]\n",
      "333: [D loss: 0.662651, acc: 0.644531]  [A loss: 0.750574, acc: 0.355469]\n",
      "334: [D loss: 0.705520, acc: 0.513672]  [A loss: 1.192298, acc: 0.000000]\n",
      "335: [D loss: 0.677019, acc: 0.574219]  [A loss: 0.657507, acc: 0.625000]\n",
      "336: [D loss: 0.732034, acc: 0.503906]  [A loss: 1.147939, acc: 0.000000]\n",
      "337: [D loss: 0.664691, acc: 0.578125]  [A loss: 0.674389, acc: 0.574219]\n",
      "338: [D loss: 0.760761, acc: 0.501953]  [A loss: 1.017354, acc: 0.007812]\n",
      "339: [D loss: 0.676910, acc: 0.568359]  [A loss: 0.783091, acc: 0.257812]\n",
      "340: [D loss: 0.684805, acc: 0.533203]  [A loss: 0.959662, acc: 0.015625]\n",
      "341: [D loss: 0.685773, acc: 0.554688]  [A loss: 0.778675, acc: 0.230469]\n",
      "342: [D loss: 0.678595, acc: 0.546875]  [A loss: 0.969377, acc: 0.015625]\n",
      "343: [D loss: 0.671421, acc: 0.599609]  [A loss: 0.794320, acc: 0.273438]\n",
      "344: [D loss: 0.709881, acc: 0.531250]  [A loss: 1.039153, acc: 0.015625]\n",
      "345: [D loss: 0.669246, acc: 0.605469]  [A loss: 0.727361, acc: 0.429688]\n",
      "346: [D loss: 0.693082, acc: 0.521484]  [A loss: 1.112777, acc: 0.003906]\n",
      "347: [D loss: 0.675462, acc: 0.558594]  [A loss: 0.724713, acc: 0.406250]\n",
      "348: [D loss: 0.708533, acc: 0.515625]  [A loss: 1.143548, acc: 0.000000]\n",
      "349: [D loss: 0.675529, acc: 0.558594]  [A loss: 0.654224, acc: 0.664062]\n",
      "350: [D loss: 0.723378, acc: 0.503906]  [A loss: 1.081529, acc: 0.011719]\n",
      "351: [D loss: 0.683058, acc: 0.556641]  [A loss: 0.743237, acc: 0.320312]\n",
      "352: [D loss: 0.702184, acc: 0.519531]  [A loss: 0.996009, acc: 0.003906]\n",
      "353: [D loss: 0.672776, acc: 0.601562]  [A loss: 0.756370, acc: 0.281250]\n",
      "354: [D loss: 0.683644, acc: 0.521484]  [A loss: 1.031356, acc: 0.007812]\n",
      "355: [D loss: 0.674549, acc: 0.585938]  [A loss: 0.774172, acc: 0.281250]\n",
      "356: [D loss: 0.693190, acc: 0.542969]  [A loss: 1.032852, acc: 0.003906]\n",
      "357: [D loss: 0.670165, acc: 0.583984]  [A loss: 0.774415, acc: 0.250000]\n",
      "358: [D loss: 0.680357, acc: 0.527344]  [A loss: 1.076098, acc: 0.003906]\n",
      "359: [D loss: 0.670545, acc: 0.603516]  [A loss: 0.724979, acc: 0.437500]\n",
      "360: [D loss: 0.702913, acc: 0.517578]  [A loss: 1.121042, acc: 0.000000]\n",
      "361: [D loss: 0.673154, acc: 0.589844]  [A loss: 0.674845, acc: 0.593750]\n",
      "362: [D loss: 0.697569, acc: 0.513672]  [A loss: 1.149102, acc: 0.000000]\n",
      "363: [D loss: 0.676239, acc: 0.580078]  [A loss: 0.640621, acc: 0.746094]\n",
      "364: [D loss: 0.710346, acc: 0.503906]  [A loss: 1.090571, acc: 0.003906]\n",
      "365: [D loss: 0.676511, acc: 0.578125]  [A loss: 0.718133, acc: 0.472656]\n",
      "366: [D loss: 0.704913, acc: 0.507812]  [A loss: 0.976841, acc: 0.015625]\n",
      "367: [D loss: 0.659408, acc: 0.634766]  [A loss: 0.749336, acc: 0.363281]\n",
      "368: [D loss: 0.707285, acc: 0.503906]  [A loss: 1.089525, acc: 0.000000]\n",
      "369: [D loss: 0.685784, acc: 0.560547]  [A loss: 0.757679, acc: 0.359375]\n",
      "370: [D loss: 0.710516, acc: 0.494141]  [A loss: 1.006877, acc: 0.023438]\n",
      "371: [D loss: 0.676343, acc: 0.554688]  [A loss: 0.810189, acc: 0.171875]\n",
      "372: [D loss: 0.679543, acc: 0.541016]  [A loss: 0.944956, acc: 0.046875]\n",
      "373: [D loss: 0.670549, acc: 0.632812]  [A loss: 0.829297, acc: 0.187500]\n",
      "374: [D loss: 0.676686, acc: 0.570312]  [A loss: 0.958593, acc: 0.019531]\n",
      "375: [D loss: 0.673835, acc: 0.585938]  [A loss: 0.857074, acc: 0.121094]\n",
      "376: [D loss: 0.679685, acc: 0.554688]  [A loss: 1.031872, acc: 0.023438]\n",
      "377: [D loss: 0.687099, acc: 0.554688]  [A loss: 0.801558, acc: 0.281250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378: [D loss: 0.703958, acc: 0.511719]  [A loss: 1.039520, acc: 0.011719]\n",
      "379: [D loss: 0.666192, acc: 0.617188]  [A loss: 0.729525, acc: 0.378906]\n",
      "380: [D loss: 0.697088, acc: 0.529297]  [A loss: 1.176309, acc: 0.000000]\n",
      "381: [D loss: 0.664924, acc: 0.585938]  [A loss: 0.645344, acc: 0.644531]\n",
      "382: [D loss: 0.723228, acc: 0.505859]  [A loss: 1.092407, acc: 0.007812]\n",
      "383: [D loss: 0.679240, acc: 0.566406]  [A loss: 0.694413, acc: 0.535156]\n",
      "384: [D loss: 0.696745, acc: 0.523438]  [A loss: 1.035016, acc: 0.007812]\n",
      "385: [D loss: 0.664530, acc: 0.585938]  [A loss: 0.720536, acc: 0.425781]\n",
      "386: [D loss: 0.700325, acc: 0.525391]  [A loss: 1.065550, acc: 0.000000]\n",
      "387: [D loss: 0.670880, acc: 0.621094]  [A loss: 0.702208, acc: 0.464844]\n",
      "388: [D loss: 0.691640, acc: 0.523438]  [A loss: 1.030066, acc: 0.007812]\n",
      "389: [D loss: 0.665641, acc: 0.593750]  [A loss: 0.742902, acc: 0.359375]\n",
      "390: [D loss: 0.694561, acc: 0.525391]  [A loss: 1.042897, acc: 0.007812]\n",
      "391: [D loss: 0.665469, acc: 0.628906]  [A loss: 0.767000, acc: 0.273438]\n",
      "392: [D loss: 0.702403, acc: 0.503906]  [A loss: 1.053533, acc: 0.050781]\n",
      "393: [D loss: 0.693112, acc: 0.513672]  [A loss: 0.816844, acc: 0.281250]\n",
      "394: [D loss: 0.699655, acc: 0.531250]  [A loss: 0.948682, acc: 0.039062]\n",
      "395: [D loss: 0.679887, acc: 0.578125]  [A loss: 0.829139, acc: 0.160156]\n",
      "396: [D loss: 0.684711, acc: 0.552734]  [A loss: 0.898946, acc: 0.085938]\n",
      "397: [D loss: 0.672954, acc: 0.591797]  [A loss: 0.912297, acc: 0.058594]\n",
      "398: [D loss: 0.688700, acc: 0.541016]  [A loss: 0.922685, acc: 0.046875]\n",
      "399: [D loss: 0.680933, acc: 0.568359]  [A loss: 0.904254, acc: 0.058594]\n",
      "400: [D loss: 0.670993, acc: 0.601562]  [A loss: 0.902634, acc: 0.066406]\n",
      "401: [D loss: 0.679705, acc: 0.562500]  [A loss: 0.956849, acc: 0.046875]\n",
      "402: [D loss: 0.676139, acc: 0.587891]  [A loss: 0.886369, acc: 0.070312]\n",
      "403: [D loss: 0.674223, acc: 0.560547]  [A loss: 0.996368, acc: 0.019531]\n",
      "404: [D loss: 0.671342, acc: 0.601562]  [A loss: 0.867819, acc: 0.109375]\n",
      "405: [D loss: 0.685521, acc: 0.564453]  [A loss: 1.130612, acc: 0.003906]\n",
      "406: [D loss: 0.674403, acc: 0.585938]  [A loss: 0.692930, acc: 0.500000]\n",
      "407: [D loss: 0.732703, acc: 0.503906]  [A loss: 1.369252, acc: 0.000000]\n",
      "408: [D loss: 0.695377, acc: 0.521484]  [A loss: 0.549282, acc: 0.902344]\n",
      "409: [D loss: 0.742528, acc: 0.501953]  [A loss: 1.068907, acc: 0.000000]\n",
      "410: [D loss: 0.683140, acc: 0.556641]  [A loss: 0.705283, acc: 0.492188]\n",
      "411: [D loss: 0.687195, acc: 0.523438]  [A loss: 0.905310, acc: 0.046875]\n",
      "412: [D loss: 0.665351, acc: 0.574219]  [A loss: 0.812667, acc: 0.164062]\n",
      "413: [D loss: 0.689858, acc: 0.544922]  [A loss: 0.944562, acc: 0.039062]\n",
      "414: [D loss: 0.664309, acc: 0.599609]  [A loss: 0.798886, acc: 0.238281]\n",
      "415: [D loss: 0.683377, acc: 0.554688]  [A loss: 0.976215, acc: 0.027344]\n",
      "416: [D loss: 0.674522, acc: 0.580078]  [A loss: 0.797583, acc: 0.218750]\n",
      "417: [D loss: 0.689223, acc: 0.539062]  [A loss: 1.055677, acc: 0.003906]\n",
      "418: [D loss: 0.660901, acc: 0.632812]  [A loss: 0.703297, acc: 0.484375]\n",
      "419: [D loss: 0.695085, acc: 0.515625]  [A loss: 1.076965, acc: 0.023438]\n",
      "420: [D loss: 0.678721, acc: 0.568359]  [A loss: 0.749557, acc: 0.359375]\n",
      "421: [D loss: 0.708600, acc: 0.523438]  [A loss: 1.014458, acc: 0.023438]\n",
      "422: [D loss: 0.667183, acc: 0.617188]  [A loss: 0.778930, acc: 0.277344]\n",
      "423: [D loss: 0.688808, acc: 0.539062]  [A loss: 0.995445, acc: 0.023438]\n",
      "424: [D loss: 0.676561, acc: 0.578125]  [A loss: 0.734495, acc: 0.406250]\n",
      "425: [D loss: 0.697130, acc: 0.501953]  [A loss: 1.060236, acc: 0.011719]\n",
      "426: [D loss: 0.667771, acc: 0.591797]  [A loss: 0.706197, acc: 0.492188]\n",
      "427: [D loss: 0.703846, acc: 0.529297]  [A loss: 1.132175, acc: 0.000000]\n",
      "428: [D loss: 0.673133, acc: 0.587891]  [A loss: 0.689046, acc: 0.503906]\n",
      "429: [D loss: 0.704904, acc: 0.515625]  [A loss: 1.021948, acc: 0.035156]\n",
      "430: [D loss: 0.672282, acc: 0.578125]  [A loss: 0.831782, acc: 0.179688]\n",
      "431: [D loss: 0.673905, acc: 0.564453]  [A loss: 0.979009, acc: 0.027344]\n",
      "432: [D loss: 0.679687, acc: 0.541016]  [A loss: 0.797161, acc: 0.222656]\n",
      "433: [D loss: 0.700422, acc: 0.554688]  [A loss: 1.063962, acc: 0.015625]\n",
      "434: [D loss: 0.674209, acc: 0.587891]  [A loss: 0.722050, acc: 0.382812]\n",
      "435: [D loss: 0.695502, acc: 0.525391]  [A loss: 1.064519, acc: 0.019531]\n",
      "436: [D loss: 0.676545, acc: 0.556641]  [A loss: 0.769210, acc: 0.285156]\n",
      "437: [D loss: 0.695672, acc: 0.525391]  [A loss: 0.997650, acc: 0.035156]\n",
      "438: [D loss: 0.668673, acc: 0.595703]  [A loss: 0.777953, acc: 0.261719]\n",
      "439: [D loss: 0.691555, acc: 0.505859]  [A loss: 1.053048, acc: 0.000000]\n",
      "440: [D loss: 0.669330, acc: 0.601562]  [A loss: 0.745733, acc: 0.347656]\n",
      "441: [D loss: 0.683624, acc: 0.546875]  [A loss: 1.079463, acc: 0.011719]\n",
      "442: [D loss: 0.662428, acc: 0.621094]  [A loss: 0.697038, acc: 0.492188]\n",
      "443: [D loss: 0.699606, acc: 0.523438]  [A loss: 1.175280, acc: 0.000000]\n",
      "444: [D loss: 0.682214, acc: 0.537109]  [A loss: 0.656841, acc: 0.636719]\n",
      "445: [D loss: 0.719354, acc: 0.501953]  [A loss: 1.043275, acc: 0.023438]\n",
      "446: [D loss: 0.667628, acc: 0.619141]  [A loss: 0.727992, acc: 0.417969]\n",
      "447: [D loss: 0.696823, acc: 0.517578]  [A loss: 0.963710, acc: 0.031250]\n",
      "448: [D loss: 0.668242, acc: 0.601562]  [A loss: 0.795041, acc: 0.226562]\n",
      "449: [D loss: 0.673953, acc: 0.568359]  [A loss: 1.002406, acc: 0.039062]\n",
      "450: [D loss: 0.672299, acc: 0.609375]  [A loss: 0.785857, acc: 0.238281]\n",
      "451: [D loss: 0.679870, acc: 0.523438]  [A loss: 1.045887, acc: 0.031250]\n",
      "452: [D loss: 0.660990, acc: 0.591797]  [A loss: 0.764901, acc: 0.316406]\n",
      "453: [D loss: 0.698164, acc: 0.523438]  [A loss: 1.058061, acc: 0.027344]\n",
      "454: [D loss: 0.680118, acc: 0.546875]  [A loss: 0.758809, acc: 0.343750]\n",
      "455: [D loss: 0.696378, acc: 0.507812]  [A loss: 1.031963, acc: 0.039062]\n",
      "456: [D loss: 0.674089, acc: 0.572266]  [A loss: 0.734479, acc: 0.406250]\n",
      "457: [D loss: 0.688448, acc: 0.533203]  [A loss: 1.048152, acc: 0.019531]\n",
      "458: [D loss: 0.667079, acc: 0.599609]  [A loss: 0.725898, acc: 0.437500]\n",
      "459: [D loss: 0.700047, acc: 0.533203]  [A loss: 1.132518, acc: 0.007812]\n",
      "460: [D loss: 0.679883, acc: 0.564453]  [A loss: 0.670444, acc: 0.613281]\n",
      "461: [D loss: 0.704764, acc: 0.509766]  [A loss: 1.034088, acc: 0.015625]\n",
      "462: [D loss: 0.675339, acc: 0.572266]  [A loss: 0.737575, acc: 0.355469]\n",
      "463: [D loss: 0.692665, acc: 0.529297]  [A loss: 0.969349, acc: 0.046875]\n",
      "464: [D loss: 0.674974, acc: 0.572266]  [A loss: 0.809480, acc: 0.195312]\n",
      "465: [D loss: 0.683110, acc: 0.529297]  [A loss: 0.939840, acc: 0.042969]\n",
      "466: [D loss: 0.672506, acc: 0.603516]  [A loss: 0.774200, acc: 0.277344]\n",
      "467: [D loss: 0.688470, acc: 0.554688]  [A loss: 1.004988, acc: 0.035156]\n",
      "468: [D loss: 0.671391, acc: 0.591797]  [A loss: 0.813126, acc: 0.191406]\n",
      "469: [D loss: 0.693065, acc: 0.537109]  [A loss: 1.004086, acc: 0.031250]\n",
      "470: [D loss: 0.676812, acc: 0.574219]  [A loss: 0.757175, acc: 0.312500]\n",
      "471: [D loss: 0.701713, acc: 0.517578]  [A loss: 1.050473, acc: 0.003906]\n",
      "472: [D loss: 0.671279, acc: 0.576172]  [A loss: 0.723688, acc: 0.437500]\n",
      "473: [D loss: 0.698582, acc: 0.511719]  [A loss: 1.085798, acc: 0.007812]\n",
      "474: [D loss: 0.687526, acc: 0.546875]  [A loss: 0.727596, acc: 0.402344]\n",
      "475: [D loss: 0.697579, acc: 0.527344]  [A loss: 1.047433, acc: 0.023438]\n",
      "476: [D loss: 0.679155, acc: 0.562500]  [A loss: 0.741401, acc: 0.371094]\n",
      "477: [D loss: 0.684172, acc: 0.533203]  [A loss: 0.928871, acc: 0.054688]\n",
      "478: [D loss: 0.680443, acc: 0.554688]  [A loss: 0.810023, acc: 0.246094]\n",
      "479: [D loss: 0.703334, acc: 0.535156]  [A loss: 0.967498, acc: 0.046875]\n",
      "480: [D loss: 0.679745, acc: 0.587891]  [A loss: 0.789558, acc: 0.265625]\n",
      "481: [D loss: 0.687546, acc: 0.541016]  [A loss: 0.979241, acc: 0.054688]\n",
      "482: [D loss: 0.675631, acc: 0.583984]  [A loss: 0.760345, acc: 0.351562]\n",
      "483: [D loss: 0.696742, acc: 0.527344]  [A loss: 0.975190, acc: 0.023438]\n",
      "484: [D loss: 0.668244, acc: 0.583984]  [A loss: 0.769518, acc: 0.316406]\n",
      "485: [D loss: 0.689705, acc: 0.527344]  [A loss: 1.008970, acc: 0.031250]\n",
      "486: [D loss: 0.683762, acc: 0.576172]  [A loss: 0.768792, acc: 0.359375]\n",
      "487: [D loss: 0.701279, acc: 0.533203]  [A loss: 0.950479, acc: 0.027344]\n",
      "488: [D loss: 0.683730, acc: 0.544922]  [A loss: 0.765315, acc: 0.312500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489: [D loss: 0.696788, acc: 0.531250]  [A loss: 0.992127, acc: 0.019531]\n",
      "490: [D loss: 0.671457, acc: 0.585938]  [A loss: 0.731516, acc: 0.382812]\n",
      "491: [D loss: 0.690520, acc: 0.537109]  [A loss: 1.018764, acc: 0.039062]\n",
      "492: [D loss: 0.675525, acc: 0.587891]  [A loss: 0.741880, acc: 0.355469]\n",
      "493: [D loss: 0.693150, acc: 0.529297]  [A loss: 1.008475, acc: 0.007812]\n",
      "494: [D loss: 0.677088, acc: 0.566406]  [A loss: 0.722469, acc: 0.410156]\n",
      "495: [D loss: 0.704650, acc: 0.521484]  [A loss: 1.092266, acc: 0.015625]\n",
      "496: [D loss: 0.673019, acc: 0.566406]  [A loss: 0.653826, acc: 0.648438]\n",
      "497: [D loss: 0.701775, acc: 0.511719]  [A loss: 1.024142, acc: 0.011719]\n",
      "498: [D loss: 0.673315, acc: 0.583984]  [A loss: 0.770654, acc: 0.234375]\n",
      "499: [D loss: 0.701420, acc: 0.501953]  [A loss: 0.946669, acc: 0.027344]\n",
      "500: [D loss: 0.666724, acc: 0.640625]  [A loss: 0.799185, acc: 0.218750]\n",
      "501: [D loss: 0.674765, acc: 0.548828]  [A loss: 0.939386, acc: 0.074219]\n",
      "502: [D loss: 0.678407, acc: 0.546875]  [A loss: 0.891260, acc: 0.105469]\n",
      "503: [D loss: 0.671362, acc: 0.591797]  [A loss: 0.828787, acc: 0.183594]\n",
      "504: [D loss: 0.673396, acc: 0.572266]  [A loss: 0.899898, acc: 0.085938]\n",
      "505: [D loss: 0.665743, acc: 0.603516]  [A loss: 0.944518, acc: 0.062500]\n",
      "506: [D loss: 0.674567, acc: 0.595703]  [A loss: 0.832273, acc: 0.179688]\n",
      "507: [D loss: 0.689094, acc: 0.542969]  [A loss: 0.979102, acc: 0.062500]\n",
      "508: [D loss: 0.672490, acc: 0.574219]  [A loss: 0.803402, acc: 0.269531]\n",
      "509: [D loss: 0.697233, acc: 0.505859]  [A loss: 1.064886, acc: 0.015625]\n",
      "510: [D loss: 0.671916, acc: 0.582031]  [A loss: 0.713958, acc: 0.476562]\n",
      "511: [D loss: 0.689011, acc: 0.535156]  [A loss: 1.139206, acc: 0.003906]\n",
      "512: [D loss: 0.671958, acc: 0.574219]  [A loss: 0.675182, acc: 0.570312]\n",
      "513: [D loss: 0.709968, acc: 0.513672]  [A loss: 1.116097, acc: 0.003906]\n",
      "514: [D loss: 0.671538, acc: 0.562500]  [A loss: 0.678196, acc: 0.570312]\n",
      "515: [D loss: 0.680350, acc: 0.539062]  [A loss: 0.981170, acc: 0.054688]\n",
      "516: [D loss: 0.674721, acc: 0.556641]  [A loss: 0.789855, acc: 0.285156]\n",
      "517: [D loss: 0.689070, acc: 0.550781]  [A loss: 0.962100, acc: 0.046875]\n",
      "518: [D loss: 0.664747, acc: 0.595703]  [A loss: 0.752093, acc: 0.351562]\n",
      "519: [D loss: 0.692850, acc: 0.539062]  [A loss: 1.000367, acc: 0.031250]\n",
      "520: [D loss: 0.662276, acc: 0.607422]  [A loss: 0.731988, acc: 0.398438]\n",
      "521: [D loss: 0.693581, acc: 0.531250]  [A loss: 1.046555, acc: 0.011719]\n",
      "522: [D loss: 0.663038, acc: 0.617188]  [A loss: 0.761574, acc: 0.343750]\n",
      "523: [D loss: 0.702888, acc: 0.515625]  [A loss: 1.067132, acc: 0.019531]\n",
      "524: [D loss: 0.670555, acc: 0.578125]  [A loss: 0.690456, acc: 0.554688]\n",
      "525: [D loss: 0.699206, acc: 0.505859]  [A loss: 1.035720, acc: 0.007812]\n",
      "526: [D loss: 0.672846, acc: 0.583984]  [A loss: 0.729083, acc: 0.429688]\n",
      "527: [D loss: 0.696363, acc: 0.525391]  [A loss: 1.013302, acc: 0.027344]\n",
      "528: [D loss: 0.675260, acc: 0.591797]  [A loss: 0.793410, acc: 0.269531]\n",
      "529: [D loss: 0.700011, acc: 0.521484]  [A loss: 0.947920, acc: 0.070312]\n",
      "530: [D loss: 0.664303, acc: 0.611328]  [A loss: 0.813587, acc: 0.203125]\n",
      "531: [D loss: 0.661798, acc: 0.615234]  [A loss: 0.925977, acc: 0.101562]\n",
      "532: [D loss: 0.675778, acc: 0.593750]  [A loss: 0.890975, acc: 0.109375]\n",
      "533: [D loss: 0.661271, acc: 0.615234]  [A loss: 0.915826, acc: 0.078125]\n",
      "534: [D loss: 0.673429, acc: 0.562500]  [A loss: 0.916107, acc: 0.078125]\n",
      "535: [D loss: 0.667110, acc: 0.609375]  [A loss: 0.961488, acc: 0.042969]\n",
      "536: [D loss: 0.664927, acc: 0.625000]  [A loss: 0.948630, acc: 0.062500]\n",
      "537: [D loss: 0.668823, acc: 0.611328]  [A loss: 0.935718, acc: 0.101562]\n",
      "538: [D loss: 0.678159, acc: 0.599609]  [A loss: 0.926960, acc: 0.089844]\n",
      "539: [D loss: 0.680686, acc: 0.550781]  [A loss: 0.912640, acc: 0.144531]\n",
      "540: [D loss: 0.686633, acc: 0.554688]  [A loss: 1.085007, acc: 0.003906]\n",
      "541: [D loss: 0.678562, acc: 0.572266]  [A loss: 0.725435, acc: 0.457031]\n",
      "542: [D loss: 0.726469, acc: 0.488281]  [A loss: 1.207145, acc: 0.007812]\n",
      "543: [D loss: 0.678384, acc: 0.556641]  [A loss: 0.620282, acc: 0.722656]\n",
      "544: [D loss: 0.737408, acc: 0.513672]  [A loss: 1.060786, acc: 0.011719]\n",
      "545: [D loss: 0.694991, acc: 0.501953]  [A loss: 0.742199, acc: 0.359375]\n",
      "546: [D loss: 0.695385, acc: 0.500000]  [A loss: 0.938419, acc: 0.082031]\n",
      "547: [D loss: 0.664652, acc: 0.570312]  [A loss: 0.778877, acc: 0.300781]\n",
      "548: [D loss: 0.665650, acc: 0.576172]  [A loss: 0.939136, acc: 0.058594]\n",
      "549: [D loss: 0.665076, acc: 0.613281]  [A loss: 0.884814, acc: 0.097656]\n",
      "550: [D loss: 0.676543, acc: 0.597656]  [A loss: 0.908902, acc: 0.109375]\n",
      "551: [D loss: 0.659911, acc: 0.591797]  [A loss: 0.893769, acc: 0.136719]\n",
      "552: [D loss: 0.659189, acc: 0.626953]  [A loss: 0.908222, acc: 0.128906]\n",
      "553: [D loss: 0.665465, acc: 0.595703]  [A loss: 0.975360, acc: 0.070312]\n",
      "554: [D loss: 0.676013, acc: 0.537109]  [A loss: 0.867497, acc: 0.128906]\n",
      "555: [D loss: 0.676545, acc: 0.568359]  [A loss: 1.011738, acc: 0.046875]\n",
      "556: [D loss: 0.657516, acc: 0.638672]  [A loss: 0.778969, acc: 0.316406]\n",
      "557: [D loss: 0.694305, acc: 0.535156]  [A loss: 1.164739, acc: 0.015625]\n",
      "558: [D loss: 0.672754, acc: 0.580078]  [A loss: 0.680834, acc: 0.582031]\n",
      "559: [D loss: 0.713011, acc: 0.515625]  [A loss: 1.193347, acc: 0.011719]\n",
      "560: [D loss: 0.675285, acc: 0.558594]  [A loss: 0.671845, acc: 0.554688]\n",
      "561: [D loss: 0.718916, acc: 0.503906]  [A loss: 1.130126, acc: 0.011719]\n",
      "562: [D loss: 0.684525, acc: 0.556641]  [A loss: 0.682733, acc: 0.609375]\n",
      "563: [D loss: 0.714929, acc: 0.515625]  [A loss: 0.966162, acc: 0.066406]\n",
      "564: [D loss: 0.670814, acc: 0.580078]  [A loss: 0.800818, acc: 0.226562]\n",
      "565: [D loss: 0.681033, acc: 0.568359]  [A loss: 0.916034, acc: 0.082031]\n",
      "566: [D loss: 0.668597, acc: 0.603516]  [A loss: 0.790693, acc: 0.285156]\n",
      "567: [D loss: 0.690143, acc: 0.533203]  [A loss: 0.958302, acc: 0.039062]\n",
      "568: [D loss: 0.667724, acc: 0.626953]  [A loss: 0.813840, acc: 0.210938]\n",
      "569: [D loss: 0.677400, acc: 0.550781]  [A loss: 0.923873, acc: 0.070312]\n",
      "570: [D loss: 0.663350, acc: 0.574219]  [A loss: 0.861609, acc: 0.164062]\n",
      "571: [D loss: 0.687559, acc: 0.541016]  [A loss: 0.976270, acc: 0.035156]\n",
      "572: [D loss: 0.668272, acc: 0.587891]  [A loss: 0.828605, acc: 0.210938]\n",
      "573: [D loss: 0.673882, acc: 0.574219]  [A loss: 0.931433, acc: 0.105469]\n",
      "574: [D loss: 0.656912, acc: 0.607422]  [A loss: 0.854911, acc: 0.164062]\n",
      "575: [D loss: 0.690753, acc: 0.542969]  [A loss: 1.119421, acc: 0.000000]\n",
      "576: [D loss: 0.678674, acc: 0.552734]  [A loss: 0.705337, acc: 0.511719]\n",
      "577: [D loss: 0.731429, acc: 0.509766]  [A loss: 1.218981, acc: 0.000000]\n",
      "578: [D loss: 0.677000, acc: 0.564453]  [A loss: 0.606775, acc: 0.714844]\n",
      "579: [D loss: 0.702761, acc: 0.521484]  [A loss: 1.020766, acc: 0.035156]\n",
      "580: [D loss: 0.659924, acc: 0.613281]  [A loss: 0.742778, acc: 0.355469]\n",
      "581: [D loss: 0.691252, acc: 0.544922]  [A loss: 0.949472, acc: 0.078125]\n",
      "582: [D loss: 0.671266, acc: 0.587891]  [A loss: 0.804750, acc: 0.218750]\n",
      "583: [D loss: 0.679189, acc: 0.554688]  [A loss: 0.944852, acc: 0.062500]\n",
      "584: [D loss: 0.678584, acc: 0.578125]  [A loss: 0.853417, acc: 0.144531]\n",
      "585: [D loss: 0.684822, acc: 0.574219]  [A loss: 0.904952, acc: 0.113281]\n",
      "586: [D loss: 0.668894, acc: 0.613281]  [A loss: 0.841991, acc: 0.187500]\n",
      "587: [D loss: 0.694742, acc: 0.535156]  [A loss: 0.944688, acc: 0.054688]\n",
      "588: [D loss: 0.672480, acc: 0.580078]  [A loss: 0.826149, acc: 0.222656]\n",
      "589: [D loss: 0.675007, acc: 0.554688]  [A loss: 0.918031, acc: 0.101562]\n",
      "590: [D loss: 0.678990, acc: 0.576172]  [A loss: 0.876556, acc: 0.156250]\n",
      "591: [D loss: 0.676214, acc: 0.587891]  [A loss: 0.948464, acc: 0.078125]\n",
      "592: [D loss: 0.672568, acc: 0.589844]  [A loss: 0.845949, acc: 0.203125]\n",
      "593: [D loss: 0.686543, acc: 0.552734]  [A loss: 0.993451, acc: 0.046875]\n",
      "594: [D loss: 0.668591, acc: 0.607422]  [A loss: 0.749501, acc: 0.324219]\n",
      "595: [D loss: 0.691803, acc: 0.537109]  [A loss: 1.028428, acc: 0.027344]\n",
      "596: [D loss: 0.682616, acc: 0.556641]  [A loss: 0.798407, acc: 0.242188]\n",
      "597: [D loss: 0.692295, acc: 0.529297]  [A loss: 1.118606, acc: 0.019531]\n",
      "598: [D loss: 0.668160, acc: 0.591797]  [A loss: 0.686472, acc: 0.628906]\n",
      "599: [D loss: 0.716940, acc: 0.507812]  [A loss: 1.143465, acc: 0.015625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600: [D loss: 0.688687, acc: 0.515625]  [A loss: 0.691566, acc: 0.554688]\n",
      "601: [D loss: 0.705470, acc: 0.525391]  [A loss: 0.991968, acc: 0.019531]\n",
      "602: [D loss: 0.668575, acc: 0.623047]  [A loss: 0.751980, acc: 0.316406]\n",
      "603: [D loss: 0.686301, acc: 0.546875]  [A loss: 0.908168, acc: 0.093750]\n",
      "604: [D loss: 0.674345, acc: 0.603516]  [A loss: 0.811415, acc: 0.187500]\n",
      "605: [D loss: 0.686629, acc: 0.564453]  [A loss: 0.884827, acc: 0.101562]\n",
      "606: [D loss: 0.669564, acc: 0.585938]  [A loss: 0.814564, acc: 0.218750]\n",
      "607: [D loss: 0.678633, acc: 0.554688]  [A loss: 0.985943, acc: 0.035156]\n",
      "608: [D loss: 0.672510, acc: 0.580078]  [A loss: 0.752645, acc: 0.335938]\n",
      "609: [D loss: 0.694796, acc: 0.511719]  [A loss: 0.990551, acc: 0.089844]\n",
      "610: [D loss: 0.689448, acc: 0.556641]  [A loss: 0.777921, acc: 0.289062]\n",
      "611: [D loss: 0.692786, acc: 0.523438]  [A loss: 0.974191, acc: 0.035156]\n",
      "612: [D loss: 0.674099, acc: 0.607422]  [A loss: 0.768164, acc: 0.332031]\n",
      "613: [D loss: 0.689465, acc: 0.546875]  [A loss: 0.999487, acc: 0.007812]\n",
      "614: [D loss: 0.680025, acc: 0.556641]  [A loss: 0.723117, acc: 0.425781]\n",
      "615: [D loss: 0.686276, acc: 0.535156]  [A loss: 1.025105, acc: 0.039062]\n",
      "616: [D loss: 0.663091, acc: 0.605469]  [A loss: 0.738237, acc: 0.375000]\n",
      "617: [D loss: 0.699950, acc: 0.527344]  [A loss: 1.020392, acc: 0.027344]\n",
      "618: [D loss: 0.667626, acc: 0.576172]  [A loss: 0.773605, acc: 0.300781]\n",
      "619: [D loss: 0.688936, acc: 0.525391]  [A loss: 0.932929, acc: 0.070312]\n",
      "620: [D loss: 0.679220, acc: 0.578125]  [A loss: 0.761979, acc: 0.378906]\n",
      "621: [D loss: 0.700891, acc: 0.521484]  [A loss: 0.963863, acc: 0.066406]\n",
      "622: [D loss: 0.658442, acc: 0.611328]  [A loss: 0.863207, acc: 0.117188]\n",
      "623: [D loss: 0.683713, acc: 0.521484]  [A loss: 0.876231, acc: 0.128906]\n",
      "624: [D loss: 0.668514, acc: 0.589844]  [A loss: 0.827222, acc: 0.199219]\n",
      "625: [D loss: 0.685984, acc: 0.529297]  [A loss: 0.924813, acc: 0.113281]\n",
      "626: [D loss: 0.690641, acc: 0.550781]  [A loss: 0.975024, acc: 0.035156]\n",
      "627: [D loss: 0.668047, acc: 0.587891]  [A loss: 0.804480, acc: 0.242188]\n",
      "628: [D loss: 0.686813, acc: 0.558594]  [A loss: 1.038136, acc: 0.066406]\n",
      "629: [D loss: 0.677641, acc: 0.562500]  [A loss: 0.732840, acc: 0.394531]\n",
      "630: [D loss: 0.698425, acc: 0.535156]  [A loss: 1.079888, acc: 0.011719]\n",
      "631: [D loss: 0.680927, acc: 0.562500]  [A loss: 0.667634, acc: 0.566406]\n",
      "632: [D loss: 0.714587, acc: 0.511719]  [A loss: 1.065809, acc: 0.007812]\n",
      "633: [D loss: 0.681550, acc: 0.546875]  [A loss: 0.720154, acc: 0.464844]\n",
      "634: [D loss: 0.691173, acc: 0.525391]  [A loss: 0.977679, acc: 0.085938]\n",
      "635: [D loss: 0.686749, acc: 0.562500]  [A loss: 0.735141, acc: 0.410156]\n",
      "636: [D loss: 0.688516, acc: 0.554688]  [A loss: 0.955672, acc: 0.058594]\n",
      "637: [D loss: 0.672684, acc: 0.568359]  [A loss: 0.807298, acc: 0.230469]\n",
      "638: [D loss: 0.669807, acc: 0.591797]  [A loss: 0.859696, acc: 0.156250]\n",
      "639: [D loss: 0.673543, acc: 0.593750]  [A loss: 0.956913, acc: 0.062500]\n",
      "640: [D loss: 0.676467, acc: 0.576172]  [A loss: 0.782443, acc: 0.281250]\n",
      "641: [D loss: 0.690320, acc: 0.550781]  [A loss: 0.993859, acc: 0.031250]\n",
      "642: [D loss: 0.667580, acc: 0.578125]  [A loss: 0.757234, acc: 0.300781]\n",
      "643: [D loss: 0.684718, acc: 0.560547]  [A loss: 0.984617, acc: 0.042969]\n",
      "644: [D loss: 0.661092, acc: 0.611328]  [A loss: 0.787123, acc: 0.269531]\n",
      "645: [D loss: 0.698849, acc: 0.531250]  [A loss: 0.974433, acc: 0.062500]\n",
      "646: [D loss: 0.679534, acc: 0.562500]  [A loss: 0.820053, acc: 0.156250]\n",
      "647: [D loss: 0.682021, acc: 0.554688]  [A loss: 0.902488, acc: 0.109375]\n",
      "648: [D loss: 0.669561, acc: 0.562500]  [A loss: 0.821157, acc: 0.238281]\n",
      "649: [D loss: 0.670676, acc: 0.603516]  [A loss: 0.959252, acc: 0.027344]\n",
      "650: [D loss: 0.668960, acc: 0.585938]  [A loss: 0.826888, acc: 0.207031]\n",
      "651: [D loss: 0.693147, acc: 0.541016]  [A loss: 1.095144, acc: 0.007812]\n",
      "652: [D loss: 0.678983, acc: 0.566406]  [A loss: 0.696802, acc: 0.484375]\n",
      "653: [D loss: 0.702054, acc: 0.521484]  [A loss: 1.070633, acc: 0.007812]\n",
      "654: [D loss: 0.678286, acc: 0.552734]  [A loss: 0.711605, acc: 0.484375]\n",
      "655: [D loss: 0.705000, acc: 0.503906]  [A loss: 1.008725, acc: 0.023438]\n",
      "656: [D loss: 0.676094, acc: 0.578125]  [A loss: 0.760796, acc: 0.351562]\n",
      "657: [D loss: 0.675759, acc: 0.548828]  [A loss: 0.946988, acc: 0.062500]\n",
      "658: [D loss: 0.672821, acc: 0.580078]  [A loss: 0.831938, acc: 0.183594]\n",
      "659: [D loss: 0.698712, acc: 0.517578]  [A loss: 0.942946, acc: 0.058594]\n",
      "660: [D loss: 0.673829, acc: 0.558594]  [A loss: 0.847169, acc: 0.160156]\n",
      "661: [D loss: 0.682349, acc: 0.552734]  [A loss: 0.913401, acc: 0.042969]\n",
      "662: [D loss: 0.682889, acc: 0.548828]  [A loss: 0.804669, acc: 0.257812]\n",
      "663: [D loss: 0.688324, acc: 0.537109]  [A loss: 0.943350, acc: 0.062500]\n",
      "664: [D loss: 0.671539, acc: 0.597656]  [A loss: 0.796236, acc: 0.269531]\n",
      "665: [D loss: 0.701096, acc: 0.531250]  [A loss: 1.011264, acc: 0.000000]\n",
      "666: [D loss: 0.674344, acc: 0.558594]  [A loss: 0.735030, acc: 0.378906]\n",
      "667: [D loss: 0.703984, acc: 0.511719]  [A loss: 0.979535, acc: 0.054688]\n",
      "668: [D loss: 0.672362, acc: 0.580078]  [A loss: 0.783596, acc: 0.277344]\n",
      "669: [D loss: 0.682491, acc: 0.544922]  [A loss: 0.967332, acc: 0.039062]\n",
      "670: [D loss: 0.673750, acc: 0.574219]  [A loss: 0.748325, acc: 0.386719]\n",
      "671: [D loss: 0.691444, acc: 0.531250]  [A loss: 0.944114, acc: 0.082031]\n",
      "672: [D loss: 0.657805, acc: 0.603516]  [A loss: 0.819601, acc: 0.230469]\n",
      "673: [D loss: 0.686338, acc: 0.564453]  [A loss: 0.931513, acc: 0.074219]\n",
      "674: [D loss: 0.674931, acc: 0.595703]  [A loss: 0.821805, acc: 0.214844]\n",
      "675: [D loss: 0.693159, acc: 0.509766]  [A loss: 0.964299, acc: 0.027344]\n",
      "676: [D loss: 0.669835, acc: 0.589844]  [A loss: 0.769600, acc: 0.312500]\n",
      "677: [D loss: 0.687458, acc: 0.566406]  [A loss: 0.929030, acc: 0.054688]\n",
      "678: [D loss: 0.672026, acc: 0.582031]  [A loss: 0.851043, acc: 0.171875]\n",
      "679: [D loss: 0.678042, acc: 0.560547]  [A loss: 0.943837, acc: 0.074219]\n",
      "680: [D loss: 0.668216, acc: 0.625000]  [A loss: 0.767362, acc: 0.351562]\n",
      "681: [D loss: 0.689087, acc: 0.521484]  [A loss: 1.018642, acc: 0.046875]\n",
      "682: [D loss: 0.679044, acc: 0.568359]  [A loss: 0.735865, acc: 0.371094]\n",
      "683: [D loss: 0.710648, acc: 0.513672]  [A loss: 1.053499, acc: 0.019531]\n",
      "684: [D loss: 0.677501, acc: 0.550781]  [A loss: 0.711962, acc: 0.433594]\n",
      "685: [D loss: 0.683691, acc: 0.531250]  [A loss: 1.000387, acc: 0.046875]\n",
      "686: [D loss: 0.685825, acc: 0.539062]  [A loss: 0.787672, acc: 0.242188]\n",
      "687: [D loss: 0.688790, acc: 0.541016]  [A loss: 0.898493, acc: 0.066406]\n",
      "688: [D loss: 0.672338, acc: 0.591797]  [A loss: 0.809228, acc: 0.226562]\n",
      "689: [D loss: 0.678172, acc: 0.556641]  [A loss: 0.899712, acc: 0.101562]\n",
      "690: [D loss: 0.685814, acc: 0.552734]  [A loss: 0.789151, acc: 0.265625]\n",
      "691: [D loss: 0.690245, acc: 0.542969]  [A loss: 0.957513, acc: 0.050781]\n",
      "692: [D loss: 0.677616, acc: 0.552734]  [A loss: 0.774336, acc: 0.296875]\n",
      "693: [D loss: 0.690003, acc: 0.537109]  [A loss: 0.945441, acc: 0.042969]\n",
      "694: [D loss: 0.681788, acc: 0.580078]  [A loss: 0.770902, acc: 0.281250]\n",
      "695: [D loss: 0.686051, acc: 0.570312]  [A loss: 0.918618, acc: 0.097656]\n",
      "696: [D loss: 0.675231, acc: 0.548828]  [A loss: 0.849101, acc: 0.132812]\n",
      "697: [D loss: 0.668696, acc: 0.578125]  [A loss: 0.915607, acc: 0.093750]\n",
      "698: [D loss: 0.678421, acc: 0.572266]  [A loss: 0.832650, acc: 0.175781]\n",
      "699: [D loss: 0.681803, acc: 0.564453]  [A loss: 0.889951, acc: 0.113281]\n",
      "700: [D loss: 0.669373, acc: 0.601562]  [A loss: 0.872085, acc: 0.136719]\n",
      "701: [D loss: 0.683075, acc: 0.539062]  [A loss: 0.943874, acc: 0.050781]\n",
      "702: [D loss: 0.685170, acc: 0.539062]  [A loss: 0.826792, acc: 0.156250]\n",
      "703: [D loss: 0.675187, acc: 0.558594]  [A loss: 0.923700, acc: 0.066406]\n",
      "704: [D loss: 0.672935, acc: 0.611328]  [A loss: 0.844207, acc: 0.132812]\n",
      "705: [D loss: 0.681843, acc: 0.564453]  [A loss: 1.018218, acc: 0.042969]\n",
      "706: [D loss: 0.679705, acc: 0.558594]  [A loss: 0.810913, acc: 0.222656]\n",
      "707: [D loss: 0.677606, acc: 0.560547]  [A loss: 1.028470, acc: 0.003906]\n",
      "708: [D loss: 0.667029, acc: 0.589844]  [A loss: 0.663151, acc: 0.621094]\n",
      "709: [D loss: 0.712277, acc: 0.511719]  [A loss: 1.119887, acc: 0.003906]\n",
      "710: [D loss: 0.680163, acc: 0.583984]  [A loss: 0.684926, acc: 0.535156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711: [D loss: 0.699129, acc: 0.517578]  [A loss: 0.969098, acc: 0.050781]\n",
      "712: [D loss: 0.671536, acc: 0.570312]  [A loss: 0.761137, acc: 0.335938]\n",
      "713: [D loss: 0.694625, acc: 0.541016]  [A loss: 0.905750, acc: 0.082031]\n",
      "714: [D loss: 0.656203, acc: 0.611328]  [A loss: 0.772484, acc: 0.351562]\n",
      "715: [D loss: 0.682411, acc: 0.585938]  [A loss: 0.903458, acc: 0.101562]\n",
      "716: [D loss: 0.668128, acc: 0.564453]  [A loss: 0.839174, acc: 0.183594]\n",
      "717: [D loss: 0.682747, acc: 0.572266]  [A loss: 0.911963, acc: 0.070312]\n",
      "718: [D loss: 0.677245, acc: 0.570312]  [A loss: 0.813604, acc: 0.167969]\n",
      "719: [D loss: 0.688557, acc: 0.550781]  [A loss: 0.919889, acc: 0.074219]\n",
      "720: [D loss: 0.688269, acc: 0.537109]  [A loss: 0.880708, acc: 0.093750]\n",
      "721: [D loss: 0.673473, acc: 0.582031]  [A loss: 0.845485, acc: 0.171875]\n",
      "722: [D loss: 0.669599, acc: 0.613281]  [A loss: 0.850579, acc: 0.152344]\n",
      "723: [D loss: 0.672576, acc: 0.605469]  [A loss: 0.915459, acc: 0.101562]\n",
      "724: [D loss: 0.677919, acc: 0.595703]  [A loss: 0.825057, acc: 0.167969]\n",
      "725: [D loss: 0.685928, acc: 0.539062]  [A loss: 0.951113, acc: 0.066406]\n",
      "726: [D loss: 0.676098, acc: 0.601562]  [A loss: 0.786710, acc: 0.269531]\n",
      "727: [D loss: 0.669923, acc: 0.568359]  [A loss: 0.968175, acc: 0.078125]\n",
      "728: [D loss: 0.674625, acc: 0.587891]  [A loss: 0.836061, acc: 0.160156]\n",
      "729: [D loss: 0.679248, acc: 0.574219]  [A loss: 0.939506, acc: 0.054688]\n",
      "730: [D loss: 0.675107, acc: 0.568359]  [A loss: 0.815287, acc: 0.230469]\n",
      "731: [D loss: 0.685744, acc: 0.544922]  [A loss: 0.936107, acc: 0.074219]\n",
      "732: [D loss: 0.670088, acc: 0.595703]  [A loss: 0.800075, acc: 0.265625]\n",
      "733: [D loss: 0.700133, acc: 0.509766]  [A loss: 1.011178, acc: 0.035156]\n",
      "734: [D loss: 0.665029, acc: 0.595703]  [A loss: 0.733826, acc: 0.433594]\n",
      "735: [D loss: 0.709662, acc: 0.525391]  [A loss: 0.984997, acc: 0.035156]\n",
      "736: [D loss: 0.674663, acc: 0.583984]  [A loss: 0.714515, acc: 0.441406]\n",
      "737: [D loss: 0.697404, acc: 0.515625]  [A loss: 0.977455, acc: 0.042969]\n",
      "738: [D loss: 0.666556, acc: 0.603516]  [A loss: 0.735105, acc: 0.390625]\n",
      "739: [D loss: 0.725539, acc: 0.517578]  [A loss: 0.926474, acc: 0.042969]\n",
      "740: [D loss: 0.683582, acc: 0.558594]  [A loss: 0.832540, acc: 0.179688]\n",
      "741: [D loss: 0.688699, acc: 0.527344]  [A loss: 0.861452, acc: 0.101562]\n",
      "742: [D loss: 0.673063, acc: 0.591797]  [A loss: 0.843322, acc: 0.140625]\n",
      "743: [D loss: 0.684686, acc: 0.568359]  [A loss: 0.861766, acc: 0.156250]\n",
      "744: [D loss: 0.678011, acc: 0.574219]  [A loss: 0.929136, acc: 0.039062]\n",
      "745: [D loss: 0.674853, acc: 0.582031]  [A loss: 0.799732, acc: 0.265625]\n",
      "746: [D loss: 0.672968, acc: 0.572266]  [A loss: 0.948887, acc: 0.089844]\n",
      "747: [D loss: 0.676655, acc: 0.570312]  [A loss: 0.868370, acc: 0.167969]\n",
      "748: [D loss: 0.682420, acc: 0.552734]  [A loss: 0.866529, acc: 0.125000]\n",
      "749: [D loss: 0.681120, acc: 0.552734]  [A loss: 0.862387, acc: 0.125000]\n",
      "750: [D loss: 0.668596, acc: 0.597656]  [A loss: 0.874355, acc: 0.136719]\n",
      "751: [D loss: 0.703552, acc: 0.513672]  [A loss: 0.997542, acc: 0.003906]\n",
      "752: [D loss: 0.680966, acc: 0.558594]  [A loss: 0.757022, acc: 0.382812]\n",
      "753: [D loss: 0.683521, acc: 0.554688]  [A loss: 0.953076, acc: 0.082031]\n",
      "754: [D loss: 0.673237, acc: 0.601562]  [A loss: 0.765145, acc: 0.308594]\n",
      "755: [D loss: 0.700475, acc: 0.523438]  [A loss: 1.009024, acc: 0.027344]\n",
      "756: [D loss: 0.675083, acc: 0.576172]  [A loss: 0.722855, acc: 0.371094]\n",
      "757: [D loss: 0.682928, acc: 0.558594]  [A loss: 0.953135, acc: 0.062500]\n",
      "758: [D loss: 0.661511, acc: 0.599609]  [A loss: 0.765748, acc: 0.320312]\n",
      "759: [D loss: 0.687980, acc: 0.537109]  [A loss: 0.958973, acc: 0.050781]\n",
      "760: [D loss: 0.676687, acc: 0.564453]  [A loss: 0.775211, acc: 0.308594]\n",
      "761: [D loss: 0.686219, acc: 0.552734]  [A loss: 0.924180, acc: 0.066406]\n",
      "762: [D loss: 0.674034, acc: 0.601562]  [A loss: 0.803792, acc: 0.207031]\n",
      "763: [D loss: 0.693907, acc: 0.525391]  [A loss: 0.988590, acc: 0.035156]\n",
      "764: [D loss: 0.676153, acc: 0.556641]  [A loss: 0.756349, acc: 0.320312]\n",
      "765: [D loss: 0.713431, acc: 0.554688]  [A loss: 0.913658, acc: 0.054688]\n",
      "766: [D loss: 0.691954, acc: 0.521484]  [A loss: 0.800168, acc: 0.218750]\n",
      "767: [D loss: 0.693675, acc: 0.511719]  [A loss: 0.888501, acc: 0.082031]\n",
      "768: [D loss: 0.688899, acc: 0.541016]  [A loss: 0.852702, acc: 0.105469]\n",
      "769: [D loss: 0.675655, acc: 0.589844]  [A loss: 0.858427, acc: 0.097656]\n",
      "770: [D loss: 0.674902, acc: 0.574219]  [A loss: 0.855985, acc: 0.167969]\n",
      "771: [D loss: 0.683019, acc: 0.531250]  [A loss: 0.931617, acc: 0.054688]\n",
      "772: [D loss: 0.669857, acc: 0.583984]  [A loss: 0.785951, acc: 0.289062]\n",
      "773: [D loss: 0.680691, acc: 0.552734]  [A loss: 1.023677, acc: 0.027344]\n",
      "774: [D loss: 0.669888, acc: 0.593750]  [A loss: 0.707418, acc: 0.500000]\n",
      "775: [D loss: 0.701831, acc: 0.541016]  [A loss: 0.985067, acc: 0.031250]\n",
      "776: [D loss: 0.668223, acc: 0.593750]  [A loss: 0.729007, acc: 0.402344]\n",
      "777: [D loss: 0.700764, acc: 0.519531]  [A loss: 0.998025, acc: 0.054688]\n",
      "778: [D loss: 0.672419, acc: 0.605469]  [A loss: 0.699448, acc: 0.425781]\n",
      "779: [D loss: 0.700335, acc: 0.523438]  [A loss: 0.956884, acc: 0.027344]\n",
      "780: [D loss: 0.671031, acc: 0.597656]  [A loss: 0.749120, acc: 0.308594]\n",
      "781: [D loss: 0.691289, acc: 0.542969]  [A loss: 0.907759, acc: 0.062500]\n",
      "782: [D loss: 0.680644, acc: 0.552734]  [A loss: 0.847388, acc: 0.167969]\n",
      "783: [D loss: 0.669488, acc: 0.580078]  [A loss: 0.877786, acc: 0.078125]\n",
      "784: [D loss: 0.670991, acc: 0.572266]  [A loss: 0.848403, acc: 0.171875]\n",
      "785: [D loss: 0.677845, acc: 0.542969]  [A loss: 0.883265, acc: 0.125000]\n",
      "786: [D loss: 0.679886, acc: 0.566406]  [A loss: 0.853900, acc: 0.167969]\n",
      "787: [D loss: 0.666467, acc: 0.583984]  [A loss: 0.830181, acc: 0.199219]\n",
      "788: [D loss: 0.691562, acc: 0.548828]  [A loss: 0.892475, acc: 0.117188]\n",
      "789: [D loss: 0.669270, acc: 0.589844]  [A loss: 0.881979, acc: 0.121094]\n",
      "790: [D loss: 0.669883, acc: 0.578125]  [A loss: 0.823682, acc: 0.195312]\n",
      "791: [D loss: 0.667509, acc: 0.580078]  [A loss: 0.842021, acc: 0.210938]\n",
      "792: [D loss: 0.688311, acc: 0.556641]  [A loss: 0.911904, acc: 0.113281]\n",
      "793: [D loss: 0.690265, acc: 0.572266]  [A loss: 0.857441, acc: 0.136719]\n",
      "794: [D loss: 0.671498, acc: 0.595703]  [A loss: 0.864293, acc: 0.117188]\n",
      "795: [D loss: 0.683170, acc: 0.566406]  [A loss: 0.982896, acc: 0.050781]\n",
      "796: [D loss: 0.678817, acc: 0.576172]  [A loss: 0.744157, acc: 0.449219]\n",
      "797: [D loss: 0.692219, acc: 0.535156]  [A loss: 1.057421, acc: 0.035156]\n",
      "798: [D loss: 0.681445, acc: 0.552734]  [A loss: 0.675116, acc: 0.550781]\n",
      "799: [D loss: 0.718353, acc: 0.517578]  [A loss: 0.976394, acc: 0.023438]\n",
      "800: [D loss: 0.671453, acc: 0.587891]  [A loss: 0.782034, acc: 0.261719]\n",
      "801: [D loss: 0.691971, acc: 0.550781]  [A loss: 0.919339, acc: 0.058594]\n",
      "802: [D loss: 0.668239, acc: 0.576172]  [A loss: 0.825334, acc: 0.148438]\n",
      "803: [D loss: 0.680956, acc: 0.552734]  [A loss: 0.823374, acc: 0.187500]\n",
      "804: [D loss: 0.681453, acc: 0.556641]  [A loss: 0.874256, acc: 0.101562]\n",
      "805: [D loss: 0.674719, acc: 0.560547]  [A loss: 0.782773, acc: 0.242188]\n",
      "806: [D loss: 0.699099, acc: 0.541016]  [A loss: 0.885451, acc: 0.101562]\n",
      "807: [D loss: 0.674010, acc: 0.587891]  [A loss: 0.836652, acc: 0.152344]\n",
      "808: [D loss: 0.686376, acc: 0.552734]  [A loss: 0.898585, acc: 0.085938]\n",
      "809: [D loss: 0.683129, acc: 0.556641]  [A loss: 0.821135, acc: 0.187500]\n",
      "810: [D loss: 0.679770, acc: 0.550781]  [A loss: 0.864134, acc: 0.132812]\n",
      "811: [D loss: 0.676578, acc: 0.572266]  [A loss: 0.841631, acc: 0.152344]\n",
      "812: [D loss: 0.680106, acc: 0.564453]  [A loss: 0.861303, acc: 0.144531]\n",
      "813: [D loss: 0.660356, acc: 0.607422]  [A loss: 0.847780, acc: 0.195312]\n",
      "814: [D loss: 0.673657, acc: 0.568359]  [A loss: 0.899669, acc: 0.074219]\n",
      "815: [D loss: 0.678679, acc: 0.580078]  [A loss: 0.798221, acc: 0.253906]\n",
      "816: [D loss: 0.687659, acc: 0.550781]  [A loss: 0.995901, acc: 0.031250]\n",
      "817: [D loss: 0.668101, acc: 0.589844]  [A loss: 0.737624, acc: 0.406250]\n",
      "818: [D loss: 0.705657, acc: 0.525391]  [A loss: 0.974072, acc: 0.039062]\n",
      "819: [D loss: 0.688797, acc: 0.535156]  [A loss: 0.744095, acc: 0.335938]\n",
      "820: [D loss: 0.679986, acc: 0.548828]  [A loss: 0.971495, acc: 0.042969]\n",
      "821: [D loss: 0.669105, acc: 0.578125]  [A loss: 0.775716, acc: 0.257812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822: [D loss: 0.705781, acc: 0.507812]  [A loss: 0.891662, acc: 0.078125]\n",
      "823: [D loss: 0.677051, acc: 0.591797]  [A loss: 0.813603, acc: 0.156250]\n",
      "824: [D loss: 0.682342, acc: 0.550781]  [A loss: 0.833179, acc: 0.230469]\n",
      "825: [D loss: 0.680048, acc: 0.570312]  [A loss: 0.876840, acc: 0.144531]\n",
      "826: [D loss: 0.700995, acc: 0.501953]  [A loss: 0.907885, acc: 0.082031]\n",
      "827: [D loss: 0.680440, acc: 0.564453]  [A loss: 0.832280, acc: 0.125000]\n",
      "828: [D loss: 0.682659, acc: 0.568359]  [A loss: 0.913422, acc: 0.078125]\n",
      "829: [D loss: 0.666153, acc: 0.585938]  [A loss: 0.827096, acc: 0.195312]\n",
      "830: [D loss: 0.690731, acc: 0.531250]  [A loss: 0.940494, acc: 0.074219]\n",
      "831: [D loss: 0.690511, acc: 0.552734]  [A loss: 0.789193, acc: 0.242188]\n",
      "832: [D loss: 0.686825, acc: 0.558594]  [A loss: 0.947373, acc: 0.050781]\n",
      "833: [D loss: 0.666700, acc: 0.587891]  [A loss: 0.791788, acc: 0.261719]\n",
      "834: [D loss: 0.698650, acc: 0.529297]  [A loss: 0.972294, acc: 0.031250]\n",
      "835: [D loss: 0.679056, acc: 0.578125]  [A loss: 0.698074, acc: 0.542969]\n",
      "836: [D loss: 0.710893, acc: 0.509766]  [A loss: 1.025247, acc: 0.023438]\n",
      "837: [D loss: 0.679763, acc: 0.556641]  [A loss: 0.733421, acc: 0.382812]\n",
      "838: [D loss: 0.690316, acc: 0.529297]  [A loss: 0.907605, acc: 0.062500]\n",
      "839: [D loss: 0.680998, acc: 0.580078]  [A loss: 0.799481, acc: 0.210938]\n",
      "840: [D loss: 0.683099, acc: 0.539062]  [A loss: 0.840215, acc: 0.144531]\n",
      "841: [D loss: 0.678998, acc: 0.539062]  [A loss: 0.822144, acc: 0.203125]\n",
      "842: [D loss: 0.675260, acc: 0.552734]  [A loss: 0.856004, acc: 0.140625]\n",
      "843: [D loss: 0.682267, acc: 0.546875]  [A loss: 0.854444, acc: 0.136719]\n",
      "844: [D loss: 0.680102, acc: 0.574219]  [A loss: 0.877174, acc: 0.132812]\n",
      "845: [D loss: 0.679618, acc: 0.544922]  [A loss: 0.817351, acc: 0.199219]\n",
      "846: [D loss: 0.681280, acc: 0.542969]  [A loss: 0.870271, acc: 0.101562]\n",
      "847: [D loss: 0.676806, acc: 0.562500]  [A loss: 0.805379, acc: 0.199219]\n",
      "848: [D loss: 0.694499, acc: 0.542969]  [A loss: 0.941381, acc: 0.062500]\n",
      "849: [D loss: 0.680960, acc: 0.554688]  [A loss: 0.825874, acc: 0.167969]\n",
      "850: [D loss: 0.686631, acc: 0.554688]  [A loss: 0.883740, acc: 0.093750]\n",
      "851: [D loss: 0.685671, acc: 0.566406]  [A loss: 0.843251, acc: 0.148438]\n",
      "852: [D loss: 0.666006, acc: 0.589844]  [A loss: 0.893613, acc: 0.121094]\n",
      "853: [D loss: 0.684227, acc: 0.546875]  [A loss: 0.830543, acc: 0.160156]\n",
      "854: [D loss: 0.674464, acc: 0.568359]  [A loss: 0.871098, acc: 0.105469]\n",
      "855: [D loss: 0.690811, acc: 0.550781]  [A loss: 0.835767, acc: 0.179688]\n",
      "856: [D loss: 0.681687, acc: 0.546875]  [A loss: 0.909215, acc: 0.070312]\n",
      "857: [D loss: 0.671892, acc: 0.601562]  [A loss: 0.815212, acc: 0.187500]\n",
      "858: [D loss: 0.686906, acc: 0.562500]  [A loss: 0.981630, acc: 0.058594]\n",
      "859: [D loss: 0.670272, acc: 0.554688]  [A loss: 0.707735, acc: 0.496094]\n",
      "860: [D loss: 0.692534, acc: 0.523438]  [A loss: 0.967187, acc: 0.074219]\n",
      "861: [D loss: 0.679798, acc: 0.548828]  [A loss: 0.716598, acc: 0.453125]\n",
      "862: [D loss: 0.692566, acc: 0.548828]  [A loss: 0.962749, acc: 0.050781]\n",
      "863: [D loss: 0.670417, acc: 0.589844]  [A loss: 0.748295, acc: 0.367188]\n",
      "864: [D loss: 0.689639, acc: 0.537109]  [A loss: 0.958530, acc: 0.050781]\n",
      "865: [D loss: 0.665972, acc: 0.597656]  [A loss: 0.738545, acc: 0.378906]\n",
      "866: [D loss: 0.713811, acc: 0.507812]  [A loss: 0.930468, acc: 0.093750]\n",
      "867: [D loss: 0.701547, acc: 0.517578]  [A loss: 0.780117, acc: 0.269531]\n",
      "868: [D loss: 0.697883, acc: 0.525391]  [A loss: 0.878632, acc: 0.093750]\n",
      "869: [D loss: 0.689198, acc: 0.539062]  [A loss: 0.812498, acc: 0.167969]\n",
      "870: [D loss: 0.690525, acc: 0.515625]  [A loss: 0.865358, acc: 0.101562]\n",
      "871: [D loss: 0.693091, acc: 0.537109]  [A loss: 0.845079, acc: 0.128906]\n",
      "872: [D loss: 0.684784, acc: 0.546875]  [A loss: 0.802405, acc: 0.210938]\n",
      "873: [D loss: 0.679501, acc: 0.578125]  [A loss: 0.866143, acc: 0.121094]\n",
      "874: [D loss: 0.671737, acc: 0.591797]  [A loss: 0.795927, acc: 0.242188]\n",
      "875: [D loss: 0.689457, acc: 0.541016]  [A loss: 0.937656, acc: 0.066406]\n",
      "876: [D loss: 0.677763, acc: 0.568359]  [A loss: 0.761184, acc: 0.320312]\n",
      "877: [D loss: 0.701745, acc: 0.552734]  [A loss: 0.964069, acc: 0.027344]\n",
      "878: [D loss: 0.678832, acc: 0.568359]  [A loss: 0.734571, acc: 0.375000]\n",
      "879: [D loss: 0.695186, acc: 0.529297]  [A loss: 0.939813, acc: 0.039062]\n",
      "880: [D loss: 0.673485, acc: 0.578125]  [A loss: 0.733525, acc: 0.437500]\n",
      "881: [D loss: 0.702262, acc: 0.513672]  [A loss: 0.914647, acc: 0.078125]\n",
      "882: [D loss: 0.675272, acc: 0.558594]  [A loss: 0.793430, acc: 0.230469]\n",
      "883: [D loss: 0.684679, acc: 0.550781]  [A loss: 0.923954, acc: 0.085938]\n",
      "884: [D loss: 0.671047, acc: 0.601562]  [A loss: 0.776057, acc: 0.265625]\n",
      "885: [D loss: 0.688382, acc: 0.529297]  [A loss: 0.900196, acc: 0.082031]\n",
      "886: [D loss: 0.673699, acc: 0.601562]  [A loss: 0.776411, acc: 0.273438]\n",
      "887: [D loss: 0.693765, acc: 0.517578]  [A loss: 0.887208, acc: 0.074219]\n",
      "888: [D loss: 0.682127, acc: 0.556641]  [A loss: 0.818560, acc: 0.207031]\n",
      "889: [D loss: 0.671986, acc: 0.595703]  [A loss: 0.840006, acc: 0.179688]\n",
      "890: [D loss: 0.694516, acc: 0.558594]  [A loss: 0.848705, acc: 0.121094]\n",
      "891: [D loss: 0.686858, acc: 0.531250]  [A loss: 0.904580, acc: 0.085938]\n",
      "892: [D loss: 0.670941, acc: 0.613281]  [A loss: 0.759321, acc: 0.371094]\n",
      "893: [D loss: 0.693118, acc: 0.523438]  [A loss: 0.957680, acc: 0.058594]\n",
      "894: [D loss: 0.679268, acc: 0.574219]  [A loss: 0.870983, acc: 0.132812]\n",
      "895: [D loss: 0.677804, acc: 0.582031]  [A loss: 0.882335, acc: 0.093750]\n",
      "896: [D loss: 0.679071, acc: 0.552734]  [A loss: 0.863539, acc: 0.128906]\n",
      "897: [D loss: 0.675909, acc: 0.564453]  [A loss: 0.832902, acc: 0.175781]\n",
      "898: [D loss: 0.695216, acc: 0.544922]  [A loss: 0.889263, acc: 0.109375]\n",
      "899: [D loss: 0.679134, acc: 0.552734]  [A loss: 0.832102, acc: 0.167969]\n",
      "900: [D loss: 0.678313, acc: 0.572266]  [A loss: 0.903419, acc: 0.101562]\n",
      "901: [D loss: 0.683807, acc: 0.546875]  [A loss: 0.823180, acc: 0.183594]\n",
      "902: [D loss: 0.695393, acc: 0.521484]  [A loss: 0.928980, acc: 0.070312]\n",
      "903: [D loss: 0.675863, acc: 0.605469]  [A loss: 0.774468, acc: 0.281250]\n",
      "904: [D loss: 0.676666, acc: 0.576172]  [A loss: 0.964342, acc: 0.035156]\n",
      "905: [D loss: 0.671506, acc: 0.587891]  [A loss: 0.795657, acc: 0.250000]\n",
      "906: [D loss: 0.670441, acc: 0.583984]  [A loss: 0.933026, acc: 0.085938]\n",
      "907: [D loss: 0.677352, acc: 0.560547]  [A loss: 0.815908, acc: 0.230469]\n",
      "908: [D loss: 0.690813, acc: 0.558594]  [A loss: 0.897409, acc: 0.074219]\n",
      "909: [D loss: 0.683717, acc: 0.568359]  [A loss: 0.841993, acc: 0.140625]\n",
      "910: [D loss: 0.671729, acc: 0.552734]  [A loss: 0.923781, acc: 0.066406]\n",
      "911: [D loss: 0.675160, acc: 0.546875]  [A loss: 0.841413, acc: 0.156250]\n",
      "912: [D loss: 0.684880, acc: 0.546875]  [A loss: 0.876442, acc: 0.128906]\n",
      "913: [D loss: 0.671016, acc: 0.615234]  [A loss: 0.856477, acc: 0.152344]\n",
      "914: [D loss: 0.683528, acc: 0.556641]  [A loss: 0.866818, acc: 0.117188]\n",
      "915: [D loss: 0.681276, acc: 0.566406]  [A loss: 0.841971, acc: 0.167969]\n",
      "916: [D loss: 0.687687, acc: 0.548828]  [A loss: 0.905064, acc: 0.078125]\n",
      "917: [D loss: 0.690568, acc: 0.498047]  [A loss: 0.819031, acc: 0.203125]\n",
      "918: [D loss: 0.689395, acc: 0.533203]  [A loss: 0.985549, acc: 0.050781]\n",
      "919: [D loss: 0.669341, acc: 0.595703]  [A loss: 0.720122, acc: 0.457031]\n",
      "920: [D loss: 0.703357, acc: 0.529297]  [A loss: 1.043438, acc: 0.019531]\n",
      "921: [D loss: 0.704502, acc: 0.498047]  [A loss: 0.680666, acc: 0.582031]\n",
      "922: [D loss: 0.697901, acc: 0.511719]  [A loss: 0.979991, acc: 0.015625]\n",
      "923: [D loss: 0.683595, acc: 0.560547]  [A loss: 0.720969, acc: 0.457031]\n",
      "924: [D loss: 0.699485, acc: 0.531250]  [A loss: 0.952696, acc: 0.058594]\n",
      "925: [D loss: 0.680753, acc: 0.552734]  [A loss: 0.736807, acc: 0.402344]\n",
      "926: [D loss: 0.692918, acc: 0.527344]  [A loss: 0.950981, acc: 0.050781]\n",
      "927: [D loss: 0.683207, acc: 0.533203]  [A loss: 0.771124, acc: 0.312500]\n",
      "928: [D loss: 0.695516, acc: 0.541016]  [A loss: 0.845532, acc: 0.128906]\n",
      "929: [D loss: 0.676689, acc: 0.585938]  [A loss: 0.795499, acc: 0.238281]\n",
      "930: [D loss: 0.693690, acc: 0.517578]  [A loss: 0.929756, acc: 0.054688]\n",
      "931: [D loss: 0.674482, acc: 0.583984]  [A loss: 0.773675, acc: 0.300781]\n",
      "932: [D loss: 0.673837, acc: 0.570312]  [A loss: 0.874695, acc: 0.097656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "933: [D loss: 0.676332, acc: 0.564453]  [A loss: 0.798825, acc: 0.265625]\n",
      "934: [D loss: 0.692687, acc: 0.533203]  [A loss: 0.894084, acc: 0.085938]\n",
      "935: [D loss: 0.677252, acc: 0.587891]  [A loss: 0.746241, acc: 0.382812]\n",
      "936: [D loss: 0.688645, acc: 0.535156]  [A loss: 0.916684, acc: 0.074219]\n",
      "937: [D loss: 0.688782, acc: 0.544922]  [A loss: 0.785432, acc: 0.242188]\n",
      "938: [D loss: 0.688704, acc: 0.554688]  [A loss: 0.923495, acc: 0.039062]\n",
      "939: [D loss: 0.685874, acc: 0.552734]  [A loss: 0.780812, acc: 0.289062]\n",
      "940: [D loss: 0.688253, acc: 0.525391]  [A loss: 0.858695, acc: 0.152344]\n",
      "941: [D loss: 0.675616, acc: 0.570312]  [A loss: 0.775597, acc: 0.339844]\n",
      "942: [D loss: 0.690410, acc: 0.537109]  [A loss: 0.952196, acc: 0.078125]\n",
      "943: [D loss: 0.667540, acc: 0.615234]  [A loss: 0.706730, acc: 0.445312]\n",
      "944: [D loss: 0.714480, acc: 0.511719]  [A loss: 0.902760, acc: 0.074219]\n",
      "945: [D loss: 0.689457, acc: 0.521484]  [A loss: 0.858306, acc: 0.113281]\n",
      "946: [D loss: 0.692823, acc: 0.558594]  [A loss: 0.812044, acc: 0.183594]\n",
      "947: [D loss: 0.681108, acc: 0.546875]  [A loss: 0.874088, acc: 0.140625]\n",
      "948: [D loss: 0.676211, acc: 0.552734]  [A loss: 0.810602, acc: 0.207031]\n",
      "949: [D loss: 0.688405, acc: 0.535156]  [A loss: 0.828874, acc: 0.187500]\n",
      "950: [D loss: 0.682443, acc: 0.564453]  [A loss: 0.866265, acc: 0.113281]\n",
      "951: [D loss: 0.691889, acc: 0.537109]  [A loss: 0.808869, acc: 0.234375]\n",
      "952: [D loss: 0.696556, acc: 0.554688]  [A loss: 0.844272, acc: 0.128906]\n",
      "953: [D loss: 0.680930, acc: 0.564453]  [A loss: 0.861926, acc: 0.152344]\n",
      "954: [D loss: 0.690256, acc: 0.542969]  [A loss: 0.823640, acc: 0.160156]\n",
      "955: [D loss: 0.686988, acc: 0.529297]  [A loss: 0.868549, acc: 0.078125]\n",
      "956: [D loss: 0.685256, acc: 0.531250]  [A loss: 0.835446, acc: 0.183594]\n",
      "957: [D loss: 0.679415, acc: 0.560547]  [A loss: 0.877775, acc: 0.125000]\n",
      "958: [D loss: 0.705009, acc: 0.494141]  [A loss: 0.884678, acc: 0.058594]\n",
      "959: [D loss: 0.679846, acc: 0.568359]  [A loss: 0.773712, acc: 0.332031]\n",
      "960: [D loss: 0.706689, acc: 0.500000]  [A loss: 0.990424, acc: 0.035156]\n",
      "961: [D loss: 0.682066, acc: 0.548828]  [A loss: 0.729072, acc: 0.382812]\n",
      "962: [D loss: 0.729322, acc: 0.507812]  [A loss: 0.972757, acc: 0.015625]\n",
      "963: [D loss: 0.682171, acc: 0.552734]  [A loss: 0.773495, acc: 0.250000]\n",
      "964: [D loss: 0.703708, acc: 0.513672]  [A loss: 0.935710, acc: 0.046875]\n",
      "965: [D loss: 0.688345, acc: 0.548828]  [A loss: 0.719123, acc: 0.433594]\n",
      "966: [D loss: 0.690594, acc: 0.541016]  [A loss: 0.924398, acc: 0.062500]\n",
      "967: [D loss: 0.681832, acc: 0.546875]  [A loss: 0.810737, acc: 0.199219]\n",
      "968: [D loss: 0.690822, acc: 0.533203]  [A loss: 0.894693, acc: 0.101562]\n",
      "969: [D loss: 0.693921, acc: 0.519531]  [A loss: 0.790425, acc: 0.250000]\n",
      "970: [D loss: 0.683976, acc: 0.572266]  [A loss: 0.876549, acc: 0.093750]\n",
      "971: [D loss: 0.681950, acc: 0.542969]  [A loss: 0.786588, acc: 0.257812]\n",
      "972: [D loss: 0.689024, acc: 0.554688]  [A loss: 0.830690, acc: 0.164062]\n",
      "973: [D loss: 0.678639, acc: 0.560547]  [A loss: 0.833795, acc: 0.191406]\n",
      "974: [D loss: 0.680292, acc: 0.550781]  [A loss: 0.853459, acc: 0.128906]\n",
      "975: [D loss: 0.676821, acc: 0.591797]  [A loss: 0.861017, acc: 0.136719]\n",
      "976: [D loss: 0.683346, acc: 0.562500]  [A loss: 0.938471, acc: 0.062500]\n",
      "977: [D loss: 0.678007, acc: 0.582031]  [A loss: 0.820493, acc: 0.238281]\n",
      "978: [D loss: 0.697302, acc: 0.517578]  [A loss: 0.800249, acc: 0.238281]\n",
      "979: [D loss: 0.686185, acc: 0.535156]  [A loss: 0.935156, acc: 0.070312]\n",
      "980: [D loss: 0.699360, acc: 0.507812]  [A loss: 0.722795, acc: 0.437500]\n",
      "981: [D loss: 0.694050, acc: 0.513672]  [A loss: 0.999652, acc: 0.019531]\n",
      "982: [D loss: 0.677850, acc: 0.556641]  [A loss: 0.676082, acc: 0.566406]\n",
      "983: [D loss: 0.718406, acc: 0.511719]  [A loss: 0.962331, acc: 0.031250]\n",
      "984: [D loss: 0.683573, acc: 0.550781]  [A loss: 0.716000, acc: 0.441406]\n",
      "985: [D loss: 0.698840, acc: 0.511719]  [A loss: 0.906908, acc: 0.058594]\n",
      "986: [D loss: 0.678458, acc: 0.572266]  [A loss: 0.761400, acc: 0.320312]\n",
      "987: [D loss: 0.680876, acc: 0.535156]  [A loss: 0.889285, acc: 0.097656]\n",
      "988: [D loss: 0.686578, acc: 0.572266]  [A loss: 0.741895, acc: 0.429688]\n",
      "989: [D loss: 0.690714, acc: 0.533203]  [A loss: 0.906592, acc: 0.085938]\n",
      "990: [D loss: 0.688236, acc: 0.535156]  [A loss: 0.759054, acc: 0.351562]\n",
      "991: [D loss: 0.706361, acc: 0.505859]  [A loss: 0.902998, acc: 0.046875]\n",
      "992: [D loss: 0.683438, acc: 0.548828]  [A loss: 0.750038, acc: 0.359375]\n",
      "993: [D loss: 0.698108, acc: 0.511719]  [A loss: 0.886858, acc: 0.109375]\n",
      "994: [D loss: 0.683521, acc: 0.550781]  [A loss: 0.773085, acc: 0.285156]\n",
      "995: [D loss: 0.714900, acc: 0.521484]  [A loss: 0.875736, acc: 0.082031]\n",
      "996: [D loss: 0.682784, acc: 0.550781]  [A loss: 0.792000, acc: 0.187500]\n",
      "997: [D loss: 0.695333, acc: 0.531250]  [A loss: 0.862006, acc: 0.093750]\n",
      "998: [D loss: 0.677632, acc: 0.568359]  [A loss: 0.790154, acc: 0.203125]\n",
      "999: [D loss: 0.693052, acc: 0.531250]  [A loss: 0.880051, acc: 0.085938]\n"
     ]
    }
   ],
   "source": [
    "# Initialize MNIST DCGAN and train\n",
    "mnist_dcgan = MNIST_DCGAN(X_train_keras)\n",
    "timer = ElapsedTimer()\n",
    "mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 6.3889336585998535 min \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate batch of synthetic MNIST images\n",
    "timer.elapsed_time()\n",
    "mnist_dcgan.plot_images(fake=True)\n",
    "mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
